import React, { useCallback, useEffect, useRef, useState, useMemo } from "react";
import { createClient } from "@deepgram/sdk";
import { useToast } from "@/hooks/use-toast";
import { useAuthContext } from "@/context/AuthContext";
import { useUser } from "@/components/common/UserDetailsGate";
import { sanitizeForDeepgramText } from "@/utils/deepgramSanitizer";

// Polyfill for Deepgram SDK
if (typeof window !== "undefined" && !(window as any).Buffer) {
  import("buffer").then(({ Buffer }) => {
    (window as any).Buffer = Buffer;
  });
}

const SAMPLE_RATE = 48000;
const KEEPALIVE_MS = 8000;

interface Transcript {
  sender: "user" | "agent";
  text: string;
  timestamp: number;
}

interface GeneratedPersona {
  name: string;
  role: string;
  primary_concern: string;
  business_details: string;
  about_person: string;
  business_context?: string;
  emotional_state?: string;
  pain_points?: string[];
  decision_authority?: string;
  personality_traits?: any;
  objections?: string[];
  industry_context?: string;
}

interface Props {
  scenario: {
    persona: GeneratedPersona;
    userProduct: string;
  } | null;
  onSessionStart?: () => void;
  onConversationProgressed?: (question?: string, personaData?: GeneratedPersona) => void;
  onConnectionChange?: (connected: boolean, disconnectFn?: () => void) => void;
  activeAgent?: 'coach' | 'persona';
  generatedPersona?: GeneratedPersona | null;
  roleplayStarted?: boolean;
  archetypeId?: string | null;
}

type ConversationStage = 'greeting' | 'product_question' | 'target_question' | 'awaiting_confirmation' | 'persona_generation';

interface ConversationData {
  product_service: string;
  target_market: string;
  persona_generated: boolean;
  conversation_stage: ConversationStage;
}

export const DualVoiceAgentInterface: React.FC<Props> = ({ 
  scenario, 
  onSessionStart, 
  onConversationProgressed, 
  onConnectionChange,
  activeAgent = 'coach',
  generatedPersona,
  roleplayStarted,
  archetypeId
}) => {
  const { toast } = useToast();
  const { user } = useAuthContext();
  const userDetails = useUser();

  // Get actual first name (for display/text)
  const getActualFirstName = useCallback(() => {
    const fullName = userDetails?.fullName?.trim();
    if (fullName) {
      return fullName.split(' ')[0];
    }
    const emailName = user?.email?.split('@')[0];
    return emailName || 'there';
  }, [userDetails, user]);

  // Get pronunciation name (for TTS only)
  const getUserName = useCallback(() => {
    // 1. Check if user filled out pronunciation field
    if (userDetails?.firstNamePronunciation?.trim()) {
      return userDetails.firstNamePronunciation.trim();
    }
    // 2. Fall back to actual first name
    return getActualFirstName();
  }, [userDetails, getActualFirstName]);
 
  // Build a short phone-style greeting for the persona (B2C/B2B variants)
  const buildPersonaGreeting = (persona: GeneratedPersona): string => {
    const fullName = (persona?.name || '').trim();
    const firstName = fullName.split(' ')[0] || 'there';
    const details = (persona?.business_details || persona?.business_context || '').trim();
    const role = (persona?.role || '').trim();

    // Heuristic B2B detection: presence of corporate keywords or " at " pattern
    const isB2B = /\bat\b/i.test(details) ||
                  /(CEO|CTO|COO|CFO|Founder|Owner|Director|Manager|Lead|VP|Vice President|Sales|Marketing|Ops|Operations|Engineer|Consultant|Specialist)/i.test(role) ||
                  /(Inc\.|LLC|Ltd\.|Corporation|Corp\.|Company|Enterprises|Studios|Agency)/i.test(details);

    // Try to extract a company name after " at "
    let company = '';
    const atMatch = details.match(/\bat\s+([A-Za-z0-9&'â€™\-\.\s]{2,})/i);
    if (atMatch) {
      company = atMatch[1].trim().replace(/[\.,;:]+$/, '');
    }
    if (!company && /\bcompany\b/i.test(details)) {
      company = 'the company';
    }
    if (!company) {
      company = 'our office';
    }

    if (isB2B) {
      const b2bVariants = [
        `Hello, this is ${firstName} at ${company}. How can I assist you today?`,
        `Good morning, ${company}, how may I assist you?`,
        `Good afternoon, ${company}. ${firstName} speaking.`,
      ];
      return b2bVariants[Math.floor(Math.random() * b2bVariants.length)];
    } else {
      const quirky = [
        `${firstName} speaking.`,
        `Hello?`,
        `Hi, this is ${firstName}.`,
        `Hello, ${firstName} speaking.`,
        `Youâ€™ve reached the snack hotlineâ€”kidding, itâ€™s ${firstName}.`,
      ];
      return quirky[Math.floor(Math.random() * quirky.length)];
    }
  };

  /* â€“â€“â€“â€“â€“ STATE â€“â€“â€“â€“â€“ */
  const [sdkReady, setSdkReady] = useState(false);
  const [connecting, setConnecting] = useState(false);
  const [connected, setConnected] = useState(false);
  const [transcripts, setTranscripts] = useState<Transcript[]>([]);
  const [wittyGreeting, setWittyGreeting] = useState<string | null>(null);
  const [conversationData, setConversationData] = useState<ConversationData>({
    product_service: '',
    target_market: '',
    persona_generated: false,
    conversation_stage: 'greeting'
  });
  const [isGeneratingPersona, setIsGeneratingPersona] = useState(false);
  const [currentUserResponse, setCurrentUserResponse] = useState('');
  const [cleanupInProgress, setCleanupInProgress] = useState(false);
  const [muted, setMuted] = useState(false);

  /* â€“â€“â€“â€“â€“ REFS â€“â€“â€“â€“â€“ */
  const dgClientRef = useRef<ReturnType<typeof createClient> | null>(null);
  const agentRef = useRef<any>(null);
  const micStreamRef = useRef<MediaStream | null>(null);
  const ctxRef = useRef<AudioContext | null>(null);
  const speakerCtxRef = useRef<AudioContext | null>(null);
  const keepAliveId = useRef<number | null>(null);
  const workletNodeRef = useRef<AudioWorkletNode | null>(null);
  
  let playHead = 0;

  /* â€“â€“â€“â€“â€“ HELPERS â€“â€“â€“â€“â€“ */
  const log = (msg: string, ...extra: unknown[]) => {
    // Smart logging - only show critical events
    const skipPatterns = [
      'ğŸ”‰ DG audio',
      'ğŸ”Š Scheduled TTS', 
      'ğŸ™ï¸ Mic chunk',
      'ğŸ” Low RMS debug',
      'ğŸ“¤ Sent',
      'ğŸ“¤ sent',
      'ğŸ¤ Worklet message',
      'ğŸ¤ Raw float samples'
    ];
    
    // Skip repetitive/noisy logs
    if (skipPatterns.some(pattern => msg.includes(pattern))) {
      return;
    }
    
    // Only log these important events
    const importantPatterns = [
      'ğŸ“¡ DG â†’ UserStarted', 'ğŸ“¡ DG â†’ ConversationText', // Speech events
      'ğŸ“¡ DG â†’ AgentThinking', 'ğŸ“¡ DG â†’ AgentStarted', // Agent events
      'ğŸ™ï¸ Mic â†’', // Mic setup complete
      'ğŸ¤ Track', // Mic track info
      'ğŸŒ WebSocket', // Connection
      'Settings applied' // Config
    };
    
    const log = (msg: string, ...extra: unknown[]) => {
      const now = new Date();
      const time = now.toLocaleTimeString('en-US', { hour12: true });
      console.log(`[DGâ€‘UI ${time}]`, msg, ...extra);
    };

  // Fetch witty greeting on mount
  useEffect(() => {
    const fetchGreeting = async () => {
      const firstName = getActualFirstName();
      if (firstName && firstName !== 'there') {
        try {
          const response = await fetch('/api/deepgram/generate-greeting', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ name: firstName })
          });
          const data = await response.json();
          if (data.success && data.greeting) {
            setWittyGreeting(data.greeting);
            log('âœ… Witty greeting generated:', data.greeting);
          }
        } catch (error) {
          log('âŒ Failed to generate witty greeting:', error);
        }
      }
    };
    fetchGreeting();
  }, [getActualFirstName]);

  // Proper agent teardown with event listener cleanup
  const teardownAgent = useCallback((agent: any) => {
{{ ... }}
    if (!agent) return;
    
    try {
      // Remove event listeners if supported
      if (typeof agent.off === 'function') {
        agent.off();
        log("âœ… Event listeners removed");
      }
      
      // Finish the agent
      if (typeof agent.finish === 'function') {
        agent.finish();
        log("âœ… Agent finished");
      }
      
      // Close the agent
      if (typeof agent.close === 'function') {
        agent.close();
        log("âœ… Agent closed");
      }
      
    } catch (error) {
      log("âš ï¸ Error during agent teardown:", error);
    }
  }, []);

  // Complete cleanup function with proper sequencing
  const completeCleanup = useCallback(async () => {
    if (cleanupInProgress) {
      log("âš ï¸ Cleanup already in progress, skipping");
      return;
    }
    
    setCleanupInProgress(true);
    log("ğŸ§¹ Starting complete cleanup...");
    
    try {
      // Stop keepalive first
      if (keepAliveId.current) {
        clearInterval(keepAliveId.current);
        keepAliveId.current = null;
        log("âœ… Keepalive cleared");
      }
      
      // Teardown agent with proper event cleanup
      if (agentRef.current) {
        teardownAgent(agentRef.current);
        agentRef.current = null;
      }
      
      // Disconnect worklet
      if (workletNodeRef.current) {
        try {
          workletNodeRef.current.disconnect();
          workletNodeRef.current = null;
          log("âœ… Worklet disconnected");
        } catch (error) {
          log("âš ï¸ Error disconnecting worklet:", error);
        }
      }
      
      // Stop microphone stream
      if (micStreamRef.current) {
        try {
          micStreamRef.current.getTracks().forEach(track => {
            track.stop();
            log(`âœ… Stopped track: ${track.kind}`);
          });
          micStreamRef.current = null;
          log("âœ… Microphone stream stopped");
        } catch (error) {
          log("âš ï¸ Error stopping mic stream:", error);
        }
      }
      
      // Close audio contexts
      if (ctxRef.current && ctxRef.current.state !== 'closed') {
        try {
          await ctxRef.current.close();
          ctxRef.current = null;
          log("âœ… Microphone context closed");
        } catch (error) {
          log("âš ï¸ Error closing microphone context:", error);
        }
      }
      
      if (speakerCtxRef.current && speakerCtxRef.current.state !== 'closed') {
        try {
          await speakerCtxRef.current.close();
          speakerCtxRef.current = null;
          log("âœ… Speaker context closed");
        } catch (error) {
          log("âš ï¸ Error closing speaker context:", error);
        }
      }
      
      // Reset state
      setConnected(false);
      setConnecting(false);
      
      log("âœ… Complete cleanup finished");
    } finally {
      setCleanupInProgress(false);
    }
  }, [cleanupInProgress, teardownAgent]);

  // Process conversation for persona generation
  const processConversation = useCallback(async (userText: string, agentText: string) => {
    try {
      log("ğŸ”„ Processing conversation:", { userText, agentText, currentStage: conversationData.conversation_stage });
      
      // Handle user responses
      if (userText) {
        setCurrentUserResponse(prev => {
          const newResponse = prev ? `${prev} ${userText}`.trim() : userText;
          log("ğŸ“ Accumulating user response:", newResponse);
          
          setConversationData(currentData => {
            // Handle confirmation for persona generation
            if (currentData.conversation_stage === 'awaiting_confirmation') {
              const confirmationResponse = userText.toLowerCase().trim();
              if (confirmationResponse.includes('yes') || confirmationResponse.includes('yeah') || 
                  confirmationResponse.includes('sure') || confirmationResponse.includes('ok') ||
                  confirmationResponse.includes('okay') || confirmationResponse.includes('go ahead')) {
                log("âœ… User confirmed persona generation");
                
                // Start cleanup and persona generation
                setTimeout(async () => {
                  log("ğŸ”‡ Starting cleanup before persona generation");
                  await completeCleanup();
                  
                  onConversationProgressed?.("Generating Persona");
                  setIsGeneratingPersona(true);
                  
                  // Generate persona
                  try {
                    log("ğŸš€ Calling persona generation API...", {
                      product_service: currentData.product_service,
                      target_market: currentData.target_market
                    });
                    
                    const response = await fetch('/api/dual-voice/generate-persona', {
                      method: 'POST',
                      headers: { 'Content-Type': 'application/json' },
                      credentials: 'include',
                      body: JSON.stringify({
                        product_service: currentData.product_service,
                        target_market: currentData.target_market,
                        archetype_id: archetypeId
                      })
                    });
                    
                    if (!response.ok) {
                      throw new Error(`HTTP ${response.status}: ${response.statusText}`);
                    }
                    
                    const result = await response.json();
                    if (result.success) {
                      log("âœ… Persona generated successfully:", result.persona.name);
                      onConversationProgressed?.("Persona Generated", result.persona);
                    } else {
                      log("âŒ Persona generation failed:", result.error);
                      toast({ 
                        title: "Persona Generation Failed", 
                        description: result.error || "Unable to generate persona",
                        variant: "destructive" 
                      });
                    }
                  } catch (error: any) {
                    log("âŒ Error calling persona API:", error);
                    toast({ 
                      title: "Connection Error", 
                      description: `Unable to generate persona: ${error.message}`,
                      variant: "destructive" 
                    });
                  } finally {
                    setIsGeneratingPersona(false);
                  }
                }, 500);
                
                return { ...currentData, persona_generated: true, conversation_stage: 'persona_generation' as ConversationStage };
              }
            }
            
            // Capture product service response
            if (currentData.conversation_stage === 'product_question' && !currentData.product_service && newResponse && newResponse.length > 10) {
              log("ğŸ“ Capturing product service response:", newResponse);
              return { ...currentData, product_service: newResponse };
            }
            
            // Capture target market response
            if (currentData.conversation_stage === 'target_question' && !currentData.target_market && newResponse && newResponse.length > 10) {
              log("ğŸ“ Capturing target market response:", newResponse);
              const updatedData = { ...currentData, target_market: newResponse };

              // If we now have both items, immediately trigger persona generation flow (no extra confirmation)
              if (updatedData.product_service && updatedData.target_market && !updatedData.persona_generated) {
                log("âœ… Both product_service and target_market captured - auto-triggering persona generation");
                try {
                  // Nudge with the exact trigger phrase Sam was instructed to say
                  if (agentRef.current && connected) {
                    agentRef.current.injectAgentMessage("Great, give me a moment while I generate your persona!");
                  }
                } catch (e) {
                  log("âš ï¸ Failed to inject trigger phrase:", e);
                }

                // Begin cleanup and persona generation asynchronously
                setTimeout(async () => {
                  try {
                    log("ğŸ”‡ Starting cleanup before persona generation");
                    await completeCleanup();
                    onConversationProgressed?.("Generating Persona");
                    setIsGeneratingPersona(true);

                    log("ğŸš€ Calling persona generation API...", {
                      product_service: updatedData.product_service,
                      target_market: updatedData.target_market,
                    });

                    const response = await fetch('/api/dual-voice/generate-persona', {
                      method: 'POST',
                      headers: { 'Content-Type': 'application/json' },
                      credentials: 'include',
                      body: JSON.stringify({
                        product_service: updatedData.product_service,
                        target_market: updatedData.target_market,
                        archetype_id: archetypeId
                      }),
                    });

                    if (!response.ok) {
                      throw new Error(`HTTP ${response.status}: ${response.statusText}`);
                    }

                    const result = await response.json();
                    if (result.success) {
                      log("âœ… Persona generated successfully:", result.persona?.name || 'unknown');
                      onConversationProgressed?.("Persona Generated", result.persona);
                    } else {
                      log("âŒ Persona generation failed:", result.error);
                      toast({
                        title: "Persona Generation Failed",
                        description: result.error || "Unable to generate persona",
                        variant: "destructive",
                      });
                    }
                  } catch (error: any) {
                    log("âŒ Error calling persona API:", error);
                    toast({
                      title: "Connection Error",
                      description: `Unable to generate persona: ${error.message}`,
                      variant: "destructive",
                    });
                  } finally {
                    setIsGeneratingPersona(false);
                  }
                }, 600);

                return { ...updatedData, persona_generated: true, conversation_stage: 'persona_generation' as ConversationStage };
              }

              return updatedData;
            }
            
            return currentData;
          });
          
          return newResponse;
        });
      }
      
      // Handle agent responses for stage transitions
      if (agentText) {
        log("ğŸ” Checking agent text for triggers:", agentText);
        
        if (agentText.toLowerCase().includes("what is your primary product or service") || 
            agentText.toLowerCase().includes("what is your product") ||
            agentText.toLowerCase().includes("primary product")) {
          log("âœ… Detected product question");
          setConversationData(prev => ({ ...prev, conversation_stage: 'product_question' as ConversationStage }));
          setCurrentUserResponse('');
          onConversationProgressed?.("Product or Service");
        } else if (agentText.toLowerCase().includes("should i go ahead and generate") ||
                   agentText.toLowerCase().includes("should i generate")) {
          log("â“ Sam is asking for persona generation confirmation");
          setConversationData(prev => ({ ...prev, conversation_stage: 'awaiting_confirmation' as ConversationStage }));
        } else if (agentText.toLowerCase().includes("who is your ideal customer") ||
                   agentText.toLowerCase().includes("target market")) {
          log("âœ… Detected target market question");
          setConversationData(prev => ({ ...prev, conversation_stage: 'target_question' as ConversationStage }));
          setCurrentUserResponse('');
          onConversationProgressed?.("Target Market");
        }
      }
    } catch (error) {
      log("âŒ Error processing conversation:", error);
    }
  }, [conversationData, connected, completeCleanup, onConversationProgressed, toast]);

  // Initialize Deepgram SDK
  useEffect(() => {
    (async () => {
      try {
        log("ğŸ”§ Fetching Deepgram token...");
        const response = await fetch("/api/deepgram/token", { credentials: "include" });
        
        if (!response.ok) {
          throw new Error(`HTTP ${response.status}: ${response.statusText}`);
        }
        
        const data = await response.json();
        const { token, success, error } = data;
        if (!token) {
          const errorMsg = error || "Token field missing from response";
          throw new Error(errorMsg);
        }
        
        dgClientRef.current = createClient(token);
        setSdkReady(true);
        log("âœ… SDK ready");
      } catch (error) {
        log("âŒ Token fetch failed:", error);
        toast({
          title: "Deepgram Token Error",
          description: `Failed to init SDK: ${error}`,
          variant: "destructive"
        });
      }
    })();
  }, [toast]);

  // Build agent settings based on active agent type
  const buildSettings = useMemo(() => {
    const userName = getUserName();
    
    if (activeAgent === 'persona' && generatedPersona) {
      const personaPrompt = sanitizeForDeepgramText(
        `You are ${generatedPersona.name}, ${generatedPersona.role}. This is a live voice-to-voice call. Speak naturally and concisely (under ~25 words per turn). Use one question max per turn and only ask reciprocal questions when the user asks you one. Be quirky, witty, human (avoid corporate/formal). Start with a brief phone-style greeting suited to B2B/B2C context. Keep turn-taking natural, avoid lists or long explanations. Plain text only, no markdown. Stay consistent with your background. Context: ${generatedPersona.business_details}. About you: ${generatedPersona.about_person}. Primary concern: ${generatedPersona.primary_concern}.`,
        900
      );
      return {
        audio: {
          input: { encoding: "linear16", sample_rate: SAMPLE_RATE },
          output: { encoding: "linear16", sample_rate: SAMPLE_RATE }
        },
        agent: {
          language: "en",
          listen: { 
            provider: {
              type: "deepgram", 
              model: "nova-2"
            }
          },
          think: {
            provider: { type: "open_ai", model: "gpt-4o-mini" },
            prompt: personaPrompt
          },
          speak: { provider: { type: "deepgram", model: "aura-2-nova-en" } },
          greeting: buildPersonaGreeting(generatedPersona)
        }
      };

 
  } else {
    // Coach agent (Sam)
    // Keep instructions concise and single-line for Deepgram compatibility while preserving structure
    const coachPrompt = sanitizeForDeepgramText(
      `You are Sam, an expert sales training coach helping ${userName} create a buyer persona. Style: friendly, concise, conversational (under ~25 words). Goal: gather two items: 1) their primary product or service; 2) their target market/ideal customer. Flow: start with a warm greeting and ask about their product/service; then ask who their target market is; when you have BOTH items, say EXACTLY "Great, give me a moment while I generate your persona!" and then stop speaking so the system can proceed. Guidance: encourage them, ask for specifics if vague, avoid long monologues, stay on task. Plain text only, no markdown.`,
      900
    );

    return {
      audio: {
        input: { encoding: "linear16", sample_rate: SAMPLE_RATE },
        output: { encoding: "linear16", sample_rate: SAMPLE_RATE }
      },
      agent: {
        language: "en",
        listen: { 
          provider: { 
            type: "deepgram", 
            model: "nova-2"
          }
        },
        think: {
          provider: { type: "open_ai", model: "gpt-4o" },
          prompt: coachPrompt
        },
        speak: { provider: { type: "deepgram", model: "aura-2-asteria-en" } },
        greeting: wittyGreeting 
          ? `Hey! ${wittyGreeting} I'm Sam, your AI sales coach. Let's create a buyer persona for your sales training. What's your primary product or service?`
          : `Hey ${userName}! I'm Sam, your AI sales coach. Let's create a buyer persona for your sales training. What's your primary product or service?`
      }
    };
  }
}, [activeAgent, generatedPersona, getUserName]);

const startSession = async () => {
  log("â–¶ï¸ Starting session...");
  // ... (rest of the code remains the same)
    if (connecting || connected || cleanupInProgress) {
      log("âš ï¸ Session start blocked - already connecting, connected, or cleaning up");
      return;
    }
    if (!dgClientRef.current) {
      log("ğŸš« Deepgram SDK not ready yet");
      return;
    }
    
    setConnecting(true);

    try {
      // Get microphone
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: SAMPLE_RATE,
          channelCount: 1
        }
      });
      
      log("âœ… Microphone granted");
      micStreamRef.current = stream;
      
      // Debug microphone stream
      const tracks = stream.getTracks();
      tracks.forEach((track, index) => {
        log(`ğŸ¤ Track ${index}: ${track.kind}, enabled: ${track.enabled}, readyState: ${track.readyState}`);
        const settings = track.getSettings();
        log(`ğŸ¤ Track settings:`, settings);
      });
      
      // Create agent
      agentRef.current = dgClientRef.current.agent();
      const agent = agentRef.current;

      // Set up event handlers
      agent.on("Open", () => {
        log("ğŸŒ WebSocket opened, sending settings...");
        const settings = buildSettings;
        log("ğŸ”§ Agent settings:", JSON.stringify(settings, null, 2));
        agent.configure(settings);
        
        // Wrap send method for visibility (from working implementation)
        if (agentRef.current && !(agentRef.current as any).__send) {
          (agentRef.current as any).__send = agentRef.current.send.bind(agentRef.current);
          agentRef.current.send = ((buf: ArrayBuffer) => {
            log(`ğŸ“¤ sent ${buf.byteLength} bytes to DG`);
            return (agentRef.current as any).__send(buf);
          }) as any;
        }
        
        // Initialize audio playback
        initSpeaker();
        
        // Start keepalive with error handling
        keepAliveId.current = window.setInterval(() => {
          try {
            agent.keepAlive();
          } catch (error) {
            log("âŒ KeepAlive failed:", error);
            if (keepAliveId.current) {
              clearInterval(keepAliveId.current);
              keepAliveId.current = null;
            }
          }
        }, KEEPALIVE_MS);
        
        setConnected(true);
        setConnecting(false);
        onConnectionChange?.(true, completeCleanup);
      });

      agent.on("SettingsApplied", () => {
        log("âœ… Settings applied, starting microphone...");
        startMicrophonePump();
      });

      // Add comprehensive event handlers from working implementation
      agent.on("UserStartedSpeaking", () => {
        log("ğŸ“¡ DG â†’ UserStartedSpeaking ğŸ¤");
      });

      agent.on("AgentAudioDone", () => {
        log("ğŸ“¡ DG â†’ AgentAudioDone ğŸ”‡");
      });

      agent.on("AgentThinking", () => {
        log("ğŸ“¡ DG â†’ AgentThinking ğŸ¤”");
      });

      agent.on("AgentStartedSpeaking", () => {
        log("ğŸ“¡ DG â†’ AgentStartedSpeaking ğŸ—£ï¸");
      });

      agent.on("ConversationText", (msg: any) => {
        const isUser = msg.role === "user";
        const text = msg.content;
        const timestamp = Date.now();
        
        log(`ğŸ“¡ DG â†’ ConversationText [${isUser ? 'USER' : 'AGENT'}]: "${text}"`);
        
        // Keep only last 20 messages to prevent memory bloat
        setTranscripts(prev => {
          const newTranscripts = [...prev, { sender: (isUser ? "user" : "agent") as "user" | "agent", text, timestamp }];
          return newTranscripts.slice(-20);
        });
        
        // Process conversation for persona generation (coach mode only)
        if (activeAgent === 'coach') {
          if (isUser) {
            processConversation(text, '');
          } else {
            processConversation('', text);
          }
        }
      });

      agent.on("Audio", (payload: any) => {
        log(`ğŸ“¡ DG â†’ AgentAudio event received! ğŸ”Š`);
        playTTS(payload);
      });

      agent.on("Close", () => {
        log("ğŸ”Œ WebSocket closed");
        completeCleanup();
      });

      agent.on("Error", (error: any) => {
        log("âŒ Agent error:", error);
        toast({
          title: "Connection Error",
          description: `Deepgram agent error: ${error.message || error}`,
          variant: "destructive"
        });
        completeCleanup();
      });

      // Note: agent.start() doesn't exist in v4 - connection starts automatically
      onSessionStart?.();
      
    } catch (error: any) {
      log("âŒ Session start failed:", error);
      toast({
        title: "Session Start Failed",
        description: `Unable to start voice session: ${error.message}`,
        variant: "destructive"
      });
      setConnecting(false);
    }
  };

  // Initialize speaker (separate context like working implementation)
  const initSpeaker = () => {
    // Create separate speaker context - don't overwrite microphone context
    const spkCtx = new AudioContext({ sampleRate: SAMPLE_RATE });
    speakerCtxRef.current = spkCtx;
    playHead = spkCtx.currentTime + 0.05;
    spkCtx.resume().catch(() => {});
    log("ğŸ”Š Speaker initialized");
  };

  // Play TTS audio
  const playTTS = (payload: any) => {
    if (!speakerCtxRef.current) {
      log("âŒ No speaker context for TTS playback");
      return;
    }

    // 1ï¸âƒ£ Normalize to ArrayBuffer - handle Uint8Array, Buffer, etc.
    const pcmBuf: ArrayBuffer | undefined =
      payload instanceof ArrayBuffer     ? payload :
      ArrayBuffer.isView(payload)        ? payload.buffer :
      ArrayBuffer.isView(payload?.audio) ? payload.audio.buffer :
      undefined;

    if (!pcmBuf?.byteLength) {
      log("âŒ Empty or invalid audio payload");
      return;
    }

    log(`ğŸ”‰ DG audio ${pcmBuf.byteLength / 2} samples (${pcmBuf.byteLength} bytes)`);
    
    // Check if audio context is suspended
    if (speakerCtxRef.current.state === "suspended") {
      log("ğŸ”Š Resuming suspended audio context...");
      speakerCtxRef.current.resume().then(() => {
        log("âœ… Audio context resumed");
      }).catch((err) => {
        log(`âŒ Failed to resume audio context: ${err}`);
      });
    }
    
    try {
      // 2ï¸âƒ£ Convert Int16 â†’ Float32 and schedule playback
      const i16 = new Int16Array(pcmBuf);
      const f32 = Float32Array.from(i16, (v) => v / 32768);
      const buf = speakerCtxRef.current.createBuffer(1, f32.length, SAMPLE_RATE);
      buf.copyToChannel(f32, 0);

      const src = speakerCtxRef.current.createBufferSource();
      src.buffer = buf;
      src.connect(speakerCtxRef.current.destination);

      const startAt = Math.max(playHead, speakerCtxRef.current.currentTime + 0.02);
      src.start(startAt);
      playHead = startAt + buf.duration;
      
      log(`ğŸ”Š Scheduled TTS playback: ${buf.duration.toFixed(2)}s at ${startAt.toFixed(2)}s`);
      
    } catch (error) {
      log(`âŒ TTS playback failed: ${error}`);
    }
  };

    // Start microphone pump (exactly matching working implementation)
  const startMicrophonePump = async () => {
    if (!micStreamRef.current) {
      log("âŒ Missing mic stream");
      return;
    }

    try {
      // Create separate audio context for microphone (like working implementation)
      const micCtx = new AudioContext({ sampleRate: SAMPLE_RATE });
      ctxRef.current = micCtx; // Store reference for cleanup
      
      log("ğŸ”§ Adding audio worklet module...");
      await micCtx.audioWorklet.addModule('/deepgram-worklet.js');
      log("âœ… Audio worklet module added");
      
      workletNodeRef.current = new AudioWorkletNode(micCtx, 'deepgram-worklet');
      log("âœ… Audio worklet node created");
      
      let hold = new Int16Array(0);
      let loggedSamples = false;
      let messageCount = 0;
      
      workletNodeRef.current.port.onmessage = (e) => {
        const data = e.data;
        if (muted) return;

        // Debug what we're actually receiving - log first few messages regardless of size
        if (!loggedSamples) {
          log(`ğŸ” Worklet data debug: type=${typeof data}, constructor=${data?.constructor?.name}, length=${data?.length}, byteLength=${data?.byteLength}`);
          if (data instanceof ArrayBuffer) {
            const int16View = new Int16Array(data);
            const maxSample = Math.max(...int16View.map(Math.abs));
            log(`ğŸ” ArrayBuffer: ${data.byteLength} bytes, ${int16View.length} samples, max=${maxSample}`);
            // Also log raw samples like working implementation
            const rawSamples = Array.from(int16View.slice(0, 10));
            log(`ğŸ¤ Raw int16 samples (first 10): [${rawSamples.join(', ')}]`);
          }
          loggedSamples = true;
        }

        // Skip processing if data is too small or empty (reduced threshold)
        if (!data || !data.byteLength || data.byteLength < 20) {
          return;
        }

        const in16 = new Int16Array(data);
        let cat = new Int16Array(hold.length + in16.length);
        cat.set(hold);
        cat.set(in16, hold.length);

        const TARGET_SAMPLES = (SAMPLE_RATE * 30) / 1000;
        while (cat.length >= TARGET_SAMPLES) {
          const chunk = cat.slice(0, TARGET_SAMPLES);
          
          const rms = Math.sqrt(chunk.reduce((sum, sample) => sum + sample * sample, 0) / chunk.length);
          const hasAudio = rms > 100;
          
          messageCount++;
          
          if (messageCount === 1) {
            log(`ğŸ™ï¸ First audio chunk: RMS: ${Math.round(rms)} ${hasAudio ? 'ğŸ”Š AUDIO DETECTED' : 'ğŸ”‡ silence'}`);
          } else if (hasAudio && messageCount <= 10) {
            log(`ğŸ”Š Speech detected! RMS: ${Math.round(rms)}`);
          }
          
          if (rms < 50 && messageCount % 500 === 0) {
            const min = Math.min(...chunk);
            const max = Math.max(...chunk);
            log(`ğŸ” Low audio levels - RMS: ${Math.round(rms)}, Range: ${min} to ${max}`);
          }
          
          if (agentRef.current && connected) {
            agentRef.current.send(chunk.buffer);
          }
          cat = cat.slice(TARGET_SAMPLES);
        }
        hold = cat;
      };
      
      // Connect microphone stream to worklet
      micCtx.createMediaStreamSource(micStreamRef.current).connect(workletNodeRef.current);
      log(`ğŸ™ï¸ Mic â†’ DG @${SAMPLE_RATE} Hz`);
      
      // Check if microphone is working
      setTimeout(() => {
        if (messageCount === 0) {
          log("âš ï¸ No audio data received from microphone after 3 seconds");
        } else {
          log(`âœ… Microphone is working - received ${messageCount} audio messages`);
        }
      }, 3000);
      
    } catch (error) {
      log("âŒ Microphone pump failed:", error);
    }
  };

  // Stop session
  const stopSession = () => {
    log("â¹ï¸ Stopping session...");
    completeCleanup();
    onConnectionChange?.(false);
  };

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      log("ğŸ§¹ Component unmounting, cleaning up...");
      completeCleanup();
    };
  }, [completeCleanup]);

  // Handle roleplay start when activeAgent changes to persona
  useEffect(() => {
    if (activeAgent === 'persona' && roleplayStarted && !connected && generatedPersona) {
      log("ğŸ­ Switching to persona agent, starting new session...");
      onConversationProgressed?.("Roleplay Active");
      // Start session automatically when switching to persona
      setTimeout(() => {
        startSession();
      }, 1000);
    }
  }, [activeAgent, roleplayStarted, connected, generatedPersona]);

  // Render minimal interface - AnimatedLandingPage handles the UI
  return (
    <div className="w-full h-full flex items-center justify-center">
      {!connected ? (
        <button 
          onClick={startSession}
          disabled={!sdkReady || connecting || cleanupInProgress}
          className="px-6 py-3 bg-blue-600 text-white rounded-lg hover:bg-blue-700 disabled:opacity-50 disabled:cursor-not-allowed"
        >
          {connecting ? "Connecting..." : cleanupInProgress ? "Preparing..." : "Start Voice Session"}
        </button>
      ) : (
        // Connected - AnimatedLandingPage shows the End Call button, so we show nothing
        <div className="text-center text-gray-600">
          <p className="text-sm">Voice session active</p>
        </div>
      )}
    </div>
  );
}; 