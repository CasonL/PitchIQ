<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Working Dual Voice Agent Implementation</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 12px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        .header {
            background: linear-gradient(135deg, #4f46e5 0%, #7c3aed 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }
        .header h1 {
            margin: 0;
            font-size: 2.5rem;
            font-weight: 700;
        }
        .header p {
            margin: 10px 0 0 0;
            opacity: 0.9;
            font-size: 1.1rem;
        }
        .content {
            padding: 40px;
        }
        .voice-agent-card {
            background: #f8fafc;
            border: 2px solid #e2e8f0;
            border-radius: 12px;
            padding: 30px;
            margin-bottom: 30px;
        }
        .voice-agent-card h2 {
            color: #1e293b;
            margin-top: 0;
            font-size: 1.8rem;
            margin-bottom: 20px;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        .badge {
            background: #e2e8f0;
            color: #64748b;
            padding: 4px 8px;
            border-radius: 4px;
            font-size: 12px;
            font-weight: 600;
        }
        .controls {
            display: flex;
            gap: 10px;
            margin: 20px 0;
            flex-wrap: wrap;
        }
        .btn {
            padding: 10px 20px;
            border: none;
            border-radius: 6px;
            cursor: pointer;
            font-size: 14px;
            font-weight: 600;
            transition: all 0.3s ease;
            display: flex;
            align-items: center;
            gap: 8px;
        }
        .btn-primary {
            background: #10b981;
            color: white;
        }
        .btn-primary:hover {
            background: #059669;
        }
        .btn-danger {
            background: #ef4444;
            color: white;
        }
        .btn-danger:hover {
            background: #dc2626;
        }
        .btn-secondary {
            background: #6366f1;
            color: white;
        }
        .btn-secondary:hover {
            background: #4f46e5;
        }
        .btn:disabled {
            background: #9ca3af;
            cursor: not-allowed;
        }
        .transcript-area {
            background: white;
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            padding: 20px;
            min-height: 80px;
            margin: 20px 0;
            font-family: monospace;
            white-space: pre-wrap;
            font-size: 14px;
        }
        .logs-area {
            background: #1e293b;
            color: white;
            padding: 15px;
            border-radius: 8px;
            font-family: monospace;
            font-size: 12px;
            max-height: 300px;
            overflow-y: auto;
            margin-top: 20px;
            white-space: pre-wrap;
        }
        .separator {
            height: 1px;
            background: #e2e8f0;
            margin: 20px 0;
        }
        .status-badge {
            display: inline-block;
            padding: 4px 12px;
            border-radius: 20px;
            font-size: 12px;
            font-weight: 600;
            text-transform: uppercase;
        }
        .status-active {
            background: #10b981;
            color: white;
        }
        .status-connecting {
            background: #f59e0b;
            color: white;
        }
        .status-disconnected {
            background: #ef4444;
            color: white;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>🎤 Working Dual Voice Agent</h1>
            <p>Real Deepgram Voice Agent Implementation</p>
            <span class="status-badge status-disconnected" id="statusBadge">Disconnected</span>
        </div>
        
        <div class="content">
            <div class="voice-agent-card">
                <h2>
                    📞 Voice Agent (Direct API)
                    <span class="badge">Deepgram Voice Agent V1</span>
                </h2>
                
                <div class="controls" id="controls">
                    <button class="btn btn-primary" id="startBtn">
                        📞 Start Call
                    </button>
                    <button class="btn btn-danger" id="stopBtn" disabled>
                        📞 Hang up
                    </button>
                    <button class="btn btn-secondary" id="muteBtn" disabled>
                        🎤 Mute
                    </button>
                </div>
                
                <div class="separator"></div>
                
                <div class="transcript-area" id="transcript">
                    —
                </div>
                
                <div class="separator"></div>
                
                <div class="logs-area" id="logs">
                    Logs will appear here...
                </div>
            </div>
        </div>
    </div>

    <script>
        // Global variables
        let voiceAgentWS = null;
        let micStream = null;
        let micCtx = null;
        let micNode = null;
        let spkCtx = null;
        let playHead = 0;

        let connected = false;
        let connecting = false;
        let muted = false;
        let logs = [];
        let transcript = '';
        
        // Smart logging variables
        let audioChunkCount = 0;
        let lastAudioLogTime = 0;
        let lastRMSValue = 0;
        let significantAudioEvents = 0;
        
        // TTS audio logging variables
        let ttsChunkCount = 0;
        let ttsTotalBytes = 0;
        let lastTTSLogTime = 0;
        let ttsStreamActive = false;
        
        // Constants
        const MIC_RATE = 48000;
        const TTS_RATE = 48000;
        const AUDIO_LOG_INTERVAL = 2000; // Log audio stats every 2 seconds
        const RMS_THRESHOLD = 100; // Threshold for "significant" audio
        const TTS_LOG_INTERVAL = 1000; // Log TTS stats every 1 second

        
        // UI elements
        const statusBadge = document.getElementById('statusBadge');
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const muteBtn = document.getElementById('muteBtn');
        const transcriptArea = document.getElementById('transcript');
        const logsArea = document.getElementById('logs');
        
        // Logging function
        function log(message) {
            const timestamp = new Date().toLocaleTimeString();
            const logMessage = `${timestamp}  ${message}`;
            console.debug(logMessage);
            logs.push(logMessage);
            if (logs.length > 200) logs.shift(); // Keep only last 200 logs
            logsArea.textContent = logs.join('\n');
            logsArea.scrollTop = logsArea.scrollHeight;
        }
        
        // Update UI state
        function updateUI() {
            if (connecting) {
                statusBadge.textContent = 'Connecting';
                statusBadge.className = 'status-badge status-connecting';
                startBtn.disabled = true;
                stopBtn.disabled = true;
                muteBtn.disabled = true;
            } else if (connected) {
                statusBadge.textContent = 'Connected';
                statusBadge.className = 'status-badge status-active';
                startBtn.disabled = true;
                stopBtn.disabled = false;
                muteBtn.disabled = false;
                muteBtn.textContent = muted ? '🔇 Un-mute' : '🎤 Mute';
            } else {
                statusBadge.textContent = 'Disconnected';
                statusBadge.className = 'status-badge status-disconnected';
                startBtn.disabled = false;
                stopBtn.disabled = true;
                muteBtn.disabled = true;
            }
        }
        
        // Create audio worklet processor inline
        const workletCode = `
            class DeepgramWorklet extends AudioWorkletProcessor {
                constructor() {
                    super();
                    this.sampleCount = 0;
                }

                process(inputs) {
                    const input = inputs[0];
                    if (!input || !input[0] || input[0].length === 0) {
                        return true;
                    }

                    const channelCount = input.length;
                    const frameCount = input[0].length;
                    
                    const buffer = new Int16Array(frameCount);
                    
                    for (let frame = 0; frame < frameCount; frame++) {
                        let sample = 0;
                        
                        for (let channel = 0; channel < channelCount; channel++) {
                            sample += input[channel][frame];
                        }
                        
                        sample = sample / channelCount;
                        sample = sample * 1.4 * 32767;
                        buffer[frame] = Math.max(-32768, Math.min(32767, Math.round(sample)));
                    }

                    this.port.postMessage(buffer.buffer, [buffer.buffer]);
                    this.sampleCount += frameCount;
                    return true;
                }
            }
            registerProcessor('deepgram-worklet', DeepgramWorklet);
        `;
        
        // Start microphone processing
        async function startMicPump() {
            if (!micStream) return;

            micCtx = new AudioContext({ sampleRate: MIC_RATE });
            
            // Create worklet from inline code
            const blob = new Blob([workletCode], { type: 'application/javascript' });
            const workletUrl = URL.createObjectURL(blob);
            await micCtx.audioWorklet.addModule(workletUrl);
            
            if (!micCtx || micCtx.state === "closed") {
                log("❌ Audio context closed during worklet load - aborting mic setup");
                return;
            }
            
            micNode = new AudioWorkletNode(micCtx, "deepgram-worklet");

            let hold = new Int16Array(0);

            micNode.port.onmessage = (e) => {
                const data = e.data;
                if (muted || !voiceAgentWS || voiceAgentWS.readyState !== WebSocket.OPEN) return;

                const in16 = new Int16Array(data);
                let cat = new Int16Array(hold.length + in16.length);
                cat.set(hold);
                cat.set(in16, hold.length);

                const TARGET_SAMPLES = (MIC_RATE * 30) / 1000; // 30ms chunks
                while (cat.length >= TARGET_SAMPLES) {
                    const chunk = cat.slice(0, TARGET_SAMPLES);
                    
                    const rms = Math.sqrt(chunk.reduce((sum, sample) => sum + sample * sample, 0) / chunk.length);
                    const hasAudio = rms > RMS_THRESHOLD;
                    
                    // Smart logging: only log periodically or on significant changes
                    audioChunkCount++;
                    const now = Date.now();
                    
                    if (hasAudio) significantAudioEvents++;
                    
                    // Log summary every 2 seconds instead of every chunk
                    if (now - lastAudioLogTime >= AUDIO_LOG_INTERVAL) {
                        const avgRMS = Math.round(lastRMSValue);
                        const voiceActivity = significantAudioEvents > 0 ? '🔊 VOICE' : '🔇 quiet';
                        log(`🎙️ Audio: ${audioChunkCount} chunks (${(audioChunkCount * 30)}ms) | RMS: ${avgRMS} | ${voiceActivity} (${significantAudioEvents} active)`);
                        
                        // Reset counters
                        audioChunkCount = 0;
                        significantAudioEvents = 0;
                        lastAudioLogTime = now;
                    }
                    
                    lastRMSValue = rms;
                    
                    // Send audio to Voice Agent
                    voiceAgentWS.send(chunk.buffer);
                    
                    cat = cat.slice(TARGET_SAMPLES);
                }
                hold = cat;
            };
            
            micCtx.createMediaStreamSource(micStream).connect(micNode);
            log(`🎙️ Mic → DG Voice Agent @${MIC_RATE} Hz`);
            
            // Initialize smart logging timer
            lastAudioLogTime = Date.now();
        }
        
        // Initialize speaker
        function initSpeaker() {
            spkCtx = new AudioContext({ 
                sampleRate: TTS_RATE,
                latencyHint: 'interactive'
            });
            playHead = spkCtx.currentTime + 0.1; // Slightly more buffer
            spkCtx.resume().catch(() => {});
            log(`🔈 Speaker ready @${TTS_RATE}Hz`);
        }
        
        // Play TTS audio
        async function playTTS(payload) {
            if (!spkCtx) {
                log("❌ No speaker context for TTS playback");
                return;
            }

            // Handle different payload types
            let pcmBuf = null;
            
            if (payload instanceof ArrayBuffer) {
                pcmBuf = payload;
            } else if (ArrayBuffer.isView(payload)) {
                pcmBuf = payload.buffer;
            } else if (payload instanceof Blob) {
                // Convert Blob to ArrayBuffer
                pcmBuf = await payload.arrayBuffer();
            } else {
                // Skip logging for unknown types to reduce noise
                return;
            }

            if (!pcmBuf || pcmBuf.byteLength === 0) {
                // Don't log empty payloads to reduce noise
                return;
            }

            try {
                // Convert Int16 PCM to Float32 for Web Audio API
                const i16 = new Int16Array(pcmBuf);
                const f32 = new Float32Array(i16.length);
                
                // Proper Int16 to Float32 conversion with slight gain reduction to prevent clipping
                for (let i = 0; i < i16.length; i++) {
                    f32[i] = (i16[i] / 32768) * 0.8; // Reduce gain to 80% to prevent distortion
                }
                
                // Create audio buffer with exact sample rate matching
                const buf = spkCtx.createBuffer(1, f32.length, TTS_RATE);
                buf.copyToChannel(f32, 0);

                // Create buffer source with proper timing
                const src = spkCtx.createBufferSource();
                src.buffer = buf;
                
                // Add a slight low-pass filter to reduce harsh frequencies
                const filter = spkCtx.createBiquadFilter();
                filter.type = 'lowpass';
                filter.frequency.value = 8000; // Cut frequencies above 8kHz
                filter.Q.value = 0.7;
                
                // Connect: source -> filter -> destination
                src.connect(filter);
                filter.connect(spkCtx.destination);

                // Schedule playback with proper timing
                const startAt = Math.max(playHead, spkCtx.currentTime + 0.05);
                src.start(startAt);
                playHead = startAt + buf.duration;
                
            } catch (error) {
                log(`❌ TTS playback failed: ${error}`);
            }
        }
        
        // Start Voice Agent session
        async function startSession() {
            log("▶️ Starting Voice Agent session");
            if (connecting || connected) return;
            
            connecting = true;
            updateUI();

            try {
                // Get microphone with optimized settings
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: false, // Disable AGC to prevent robotic artifacts
                        sampleRate: 48000,
                        channelCount: 1,
                        googEchoCancellation: true,
                        googNoiseSuppression: true,
                        googAutoGainControl: false,
                        googHighpassFilter: false // Disable highpass filter
                    }
                });
                
                log("✅ Microphone access granted");
                micStream = stream;
                
                // Get Deepgram API key
                log("🔧 Fetching Deepgram API key...");
                const tokenRes = await fetch("/api/deepgram/token", { credentials: "include" });
                const tokenData = await tokenRes.json();
                
                if (!tokenData.success) {
                    throw new Error(tokenData.error || 'Failed to get API key');
                }
                
                // Connect to Deepgram Voice Agent API
                const wsUrl = `wss://agent.deepgram.com/v1/agent/converse`;
                log("🌐 Connecting to Deepgram Voice Agent API...");
                
                voiceAgentWS = new WebSocket(wsUrl, ['token', tokenData.token]);
                
                voiceAgentWS.onopen = () => {
                    log("✅ WebSocket connected to Voice Agent");
                    
                    // Send configuration
                    const config = {
                        type: "Settings",
                        audio: {
                            input: {
                                encoding: "linear16",
                                sample_rate: 48000
                            },
                            output: {
                                encoding: "linear16",
                                sample_rate: 48000,
                                container: "none"
                            }
                        },
                        agent: {
                            language: "en",
                            listen: {
                                provider: {
                                    type: "deepgram",
                                    model: "nova-2"
                                }
                            },
                            think: {
                                provider: {
                                    type: "open_ai",
                                    model: "gpt-4o-mini",
                                    temperature: 0.8
                                },
                                prompt: "You are a helpful AI assistant. Keep responses natural, conversational, and brief. Speak as if you're having a friendly conversation."
                            },
                            speak: {
                                provider: {
                                    type: "deepgram",
                                    model: "aura-2-asteria-en"
                                }
                            },
                            greeting: "Hello! I'm your AI assistant. How can I help you today?"
                        }
                    };
                    
                    log("📤 Sending configuration to Voice Agent");
                    voiceAgentWS.send(JSON.stringify(config));
                    
                    // Initialize audio
                    initSpeaker();
                    startMicPump();
                    
                    connected = true;
                    connecting = false;
                    updateUI();
                };
                
                voiceAgentWS.onmessage = (event) => {
                    try {
                        if (typeof event.data === 'string') {
                            // Text message (JSON)
                            const message = JSON.parse(event.data);
                            
                            // Smart logging: Only log important events
                            if (message.type === 'ConversationText') {
                                const text = message.text || '';
                                const role = message.role || 'unknown';
                                
                                if (role === 'user') {
                                    transcript += `User: ${text}\n`;
                                    log(`🗣️ User: ${text}`);
                                } else if (role === 'assistant') {
                                    transcript += `Agent: ${text}\n`;
                                    log(`🤖 Agent: ${text}`);
                                }
                                
                                transcriptArea.textContent = transcript;
                                transcriptArea.scrollTop = transcriptArea.scrollHeight;
                                
                            } else if (message.type === 'SettingsApplied') {
                                log("⚙️ Voice Agent configured successfully");
                            } else if (message.type === 'UserStartedSpeaking') {
                                log("🗣️ User speaking detected");
                            } else if (message.type === 'AgentThinking') {
                                log("🤔 Agent processing...");
                            } else if (message.type === 'Welcome') {
                                log("👋 Agent ready");
                            } else if (message.type === 'AgentAudioDone') {
                                log('📡 AgentAudioDone');
                                ttsStreamActive = false;
                                
                                // Log final TTS summary if there are remaining chunks
                                if (ttsChunkCount > 0) {
                                    const durationMs = (ttsTotalBytes / 2 / TTS_RATE * 1000).toFixed(0);
                                    log(`🎵 TTS final: ${ttsChunkCount} chunks, ${(ttsTotalBytes / 1024).toFixed(1)}KB (${durationMs}ms audio)`);
                                    ttsChunkCount = 0;
                                    ttsTotalBytes = 0;
                                }
                            } else {
                                // Log other events with more detail for debugging
                                if (message.type === 'Warning' || message.type === 'Error') {
                                    log(`⚠️ ${message.type}: ${JSON.stringify(message)}`);
                                } else {
                                    log(`📡 ${message.type}`);
                                }
                            }
                        } else {
                            // Binary message (audio) - smart TTS logging
                            const dataType = event.data.constructor.name;
                            const dataSize = event.data.byteLength || event.data.size || 0;
                            
                            if (dataSize > 0) {
                                // Smart TTS logging - aggregate chunks
                                ttsChunkCount++;
                                ttsTotalBytes += dataSize;
                                const now = Date.now();
                                
                                if (!ttsStreamActive) {
                                    ttsStreamActive = true;
                                    lastTTSLogTime = now;
                                    log(`🔊 TTS stream started`);
                                }
                                
                                // Log summary every second during TTS stream
                                if (now - lastTTSLogTime >= TTS_LOG_INTERVAL) {
                                    const durationMs = (ttsTotalBytes / 2 / TTS_RATE * 1000).toFixed(0);
                                    log(`🎵 TTS: ${ttsChunkCount} chunks, ${(ttsTotalBytes / 1024).toFixed(1)}KB (${durationMs}ms audio)`);
                                    
                                    // Reset counters for next interval
                                    ttsChunkCount = 0;
                                    ttsTotalBytes = 0;
                                    lastTTSLogTime = now;
                                }
                                
                                playTTS(event.data);
                            }
                        }
                    } catch (error) {
                        log(`❌ Error processing message: ${error}`);
                    }
                };
                
                voiceAgentWS.onerror = (error) => {
                    log(`🚨 WebSocket error: ${error}`);
                };
                
                voiceAgentWS.onclose = (event) => {
                    log(`🌐 WebSocket closed: ${event.code} ${event.reason}`);
                    cleanup();
                };
                
            } catch (e) {
                log(`❌ Failed to start session: ${e}`);
                connecting = false;
                updateUI();
            }
        }
        
        // Cleanup function
        function cleanup() {
            log("🧹 Cleaning up voice session...");
            
            if (micStream) {
                micStream.getTracks().forEach(t => t.stop());
                micStream = null;
            }
            if (micNode) {
                micNode.disconnect();
                micNode = null;
            }
            if (micCtx) {
                micCtx.close();
                micCtx = null;
            }
            if (spkCtx) {
                spkCtx.close();
                spkCtx = null;
            }
            if (voiceAgentWS) {
                voiceAgentWS.close();
                voiceAgentWS = null;
            }

            // Reset smart logging counters
            audioChunkCount = 0;
            lastAudioLogTime = 0;
            significantAudioEvents = 0;
            
            // Reset TTS logging counters
            ttsChunkCount = 0;
            ttsTotalBytes = 0;
            lastTTSLogTime = 0;
            ttsStreamActive = false;

            connected = false;
            connecting = false;
            updateUI();
            
            log("✅ Voice session ended");
        }
        
        // Stop session
        function stopSession() {
            log("⏹️ Stopping Voice Agent session");
            cleanup();
        }
        
        // Toggle mute
        function toggleMute() {
            muted = !muted;
            log(muted ? '🔇 Microphone muted' : '🔊 Microphone unmuted');
            updateUI();
        }
        
        // Initialize on page load
        document.addEventListener('DOMContentLoaded', function() {
            log('🚀 Deepgram Voice Agent Implementation loaded');
            log('📋 Ready to start voice session');
            
            // Set up event listeners
            startBtn.addEventListener('click', startSession);
            stopBtn.addEventListener('click', stopSession);
            muteBtn.addEventListener('click', toggleMute);
            
            updateUI();
        });
        
        // Cleanup on page unload
        window.addEventListener('beforeunload', function() {
            if (connected) {
                stopSession();
            }
        });
    </script>
</body>
</html> 