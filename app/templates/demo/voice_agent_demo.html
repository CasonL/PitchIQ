{% extends "demo/white_base.html" %}

{% block head %}
<!-- React dependencies for PersonaGenerationCard -->
<script src="https://unpkg.com/react@18/umd/react.production.min.js" crossorigin></script>
<script src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js" crossorigin></script>
<!-- Load white background enforcer script at earliest possible moment -->
<script src="{{ url_for('static', filename='js/white-background-enforcer.js') }}"></script>
<!-- Load Tailwind theme override script to fix React components -->
<script src="{{ url_for('static', filename='js/tailwind-theme-override.js') }}"></script>
<style id="force-white-background">
    /* Maximum override for white background */
    html, body, #main-content, main, div, section, article, aside, header, footer, nav,
    .container, .container-fluid, .row, .col, .card, .card-body, .card-header,
    .bg-gray-50, .bg-gray-100, .bg-gray-200, .bg-light,
    [class*="bg-"], [id*="voice"], [id*="demo"], [class*="voice"], [class*="demo"],
    .transcript-display, .log-display {
        background-color: white !important;
        background-image: none !important;
        background: white !important;
    }
    
    /* Override CSS variables at the root */
    :root {
        --pitchiq-white: #FFFFFF !important;
        --pitchiq-light-gray: #FFFFFF !important;
        --background-color: #FFFFFF !important;
        --bs-body-bg: #FFFFFF !important;
        --bs-light-rgb: 255,255,255 !important;
    }
</style>

<!-- Inject script into head for earliest possible execution -->
<script>
    // This script runs immediately when parsed
    (function() {
        // Create style tag with highest specificity overrides
        var style = document.createElement('style');
        style.type = 'text/css';
        style.innerHTML = `
            body { background: #FFFFFF !important; }
            #main-content { background: #FFFFFF !important; }
            .container, .container-fluid { background: #FFFFFF !important; }
            * { background-color: #FFFFFF !important; }
        `;
        
        // Insert at beginning of head for highest priority
        var head = document.head || document.getElementsByTagName('head')[0];
        if (head.firstChild) {
            head.insertBefore(style, head.firstChild);
        } else {
            head.appendChild(style);
        }
        
        // Set background image to pure white SVG
        document.body.style.backgroundImage = 'url("{{ url_for(\'static\', filename=\'images/white-background.svg\') }}")' ;
    })();
</script>
{% endblock %}

{% block title %}Deepgram Voice Agent Demo{% endblock %}

{% block extra_css %}
<!-- White background override CSS -->
<link rel="stylesheet" href="{{ url_for('static', filename='css/voice-demo-white.css') }}">

{% if force_white_background %}
<!-- Force absolute white backgrounds when parameter is set -->
<link rel="stylesheet" href="{{ url_for('static', filename='css/absolute-white-override.css') }}">
{% endif %}

<!-- Target Tailwind CSS classes in React components -->
<link rel="stylesheet" href="{{ url_for('static', filename='css/tailwind-override.css') }}">

<!-- Inline Tailwind CSS overrides for immediate effect -->
<style>
    /* Direct fix for the specific component with grey background */
    .bg-background {
        background-color: #FFFFFF !important;
        background: #FFFFFF !important;
    }
    
    .min-h-screen {
        background-color: #FFFFFF !important;
        background: #FFFFFF !important;
    }
    
    /* Target React component wrappers */
    [class*="deepgram-voice-agent"],
    [class*="deepgram-voice-agent"] > div {
        background-color: #FFFFFF !important;
        background: #FFFFFF !important;
    }
</style>

<style>
    /* Global CSS variable overrides */
    :root {
        --pitchiq-white: #FFFFFF !important;
        --pitchiq-light-gray: #FFFFFF !important;
        --background-color: #FFFFFF !important;
        --bs-body-bg: #FFFFFF !important;
        --bs-light-rgb: 255,255,255 !important;
        --bs-gray-100: #FFFFFF !important;
        --bs-gray-200: #FFFFFF !important;
    }
    
    /* Force white backgrounds on all elements */
    html, body, #main-content, main, .container, .container-fluid, .row, .col, div,
    section, article, header, footer, nav, aside, .card, .card-body, .card-header,
    .py-10, .py-3, .py-5, [class*="bg-"], [class*="demo"], .transcript-display {
        background-color: #FFFFFF !important;
        background: #FFFFFF !important;
    }
</style>

<script>
    // Execute after DOM loads to force white backgrounds
    document.addEventListener('DOMContentLoaded', function() {
        // Function to force white backgrounds
        function forceWhiteBackgrounds() {
            // Apply to document and body
            document.documentElement.style.backgroundColor = '#FFFFFF';
            document.body.style.backgroundColor = '#FFFFFF';
            
            // Apply to main content
            const mainContent = document.getElementById('main-content');
            if (mainContent) mainContent.style.backgroundColor = '#FFFFFF';
            
            // Apply to all elements that might have backgrounds
            document.querySelectorAll('div, main, section, .container, .container-fluid, .card, .card-body').forEach(el => {
                el.style.backgroundColor = '#FFFFFF';
            });
        }
        
        // Run immediately and after a delay
        forceWhiteBackgrounds();
        setTimeout(forceWhiteBackgrounds, 500);
        
        // Run whenever voice session starts (when the gray background was appearing)
        const startButton = document.getElementById('startButton');
        if (startButton) {
            startButton.addEventListener('click', function() {
                setTimeout(forceWhiteBackgrounds, 100);
                setTimeout(forceWhiteBackgrounds, 500);
                setTimeout(forceWhiteBackgrounds, 1000);
            });
        }
        
        // Run periodically to catch dynamic elements
        setInterval(forceWhiteBackgrounds, 2000);
    });
</script>
{% endblock %}

{% block styles %}
<style>
    /* Force white backgrounds directly on the body and all content */
    body {
        background-color: #FFFFFF !important;
        background: #FFFFFF !important;
    }

    body::before,
    body::after {
        background-color: #FFFFFF !important;
        background: #FFFFFF !important;
    }

    /* Target elements inserted by React */
    #root, #app, [id^="react"], [class^="react"] {
        background-color: #FFFFFF !important;
        background: #FFFFFF !important;
    }
    
    /* Force white backgrounds everywhere */
    body, html, main, #main-content {
        background-color: #FFFFFF !important;
    }
    
    .container, .container-fluid {
        background-color: #FFFFFF !important;
    }
    
    .py-10 {
        background-color: #FFFFFF !important;
    }
    
    /* Override any CSS variables that might be causing the grey background */
    :root {
        --background-color: #FFFFFF !important;
        --pitchiq-white: #FFFFFF !important;
        --pitchiq-light-gray: #FFFFFF !important;
    }
</style>
{% endblock %}

{% block styles %}
<style>
    /* Force white backgrounds everywhere */
    html, body, #main-content, main, .container, .container-fluid, .row, .col, div {
        background-color: #FFFFFF !important;
    }
    
    /* Override CSS variables */
    :root {
        --background-color: #FFFFFF !important;
        --pitchiq-white: #FFFFFF !important;
        --pitchiq-light-gray: #FFFFFF !important;
    }
    
    /* Target specific background classes */
    .bg-gray-50, .bg-light, .bg-secondary {
        background-color: #FFFFFF !important;
    }
    
    /* Make sure parent elements don't have background images */
    * {
        background-image: none !important;
    }
</style>
{% endblock %}

{% block content %}
<div id="white-background-overlay" style="position: fixed; top: 0; left: 0; right: 0; bottom: 0; width: 100%; height: 100%; background: #FFFFFF; z-index: 9999; pointer-events: none;"></div>
<script>
// Force white background on page load
document.addEventListener('DOMContentLoaded', function() {
    // Create style element
    const styleEl = document.createElement('style');
    styleEl.textContent = `
        body, html, #main-content, .container, .container-fluid, div {
            background-color: #FFFFFF !important;
            background: #FFFFFF !important;
        }
        
        :root {
            --pitchiq-white: #FFFFFF !important;
            --pitchiq-light-gray: #FFFFFF !important;
            --background-color: #FFFFFF !important;
        }
    `;
    document.head.appendChild(styleEl);
    
    // Update all elements recursively
    function makeWhite() {
        document.body.style.backgroundColor = '#FFFFFF';
        document.body.style.background = '#FFFFFF';
        
        // Force all elements to have white background
        document.querySelectorAll('*').forEach(el => {
            const style = window.getComputedStyle(el);
            const bg = style.backgroundColor;
            
            if (bg && bg !== 'rgba(0, 0, 0, 0)' && bg !== 'transparent') {
                el.style.backgroundColor = '#FFFFFF';
            }
        });
    }
    
    makeWhite();
    setInterval(makeWhite, 1000); // Run every second to catch dynamic content
    
    // When voice session starts (original issue was when session starts)
    setTimeout(() => {
        const startButton = document.getElementById('startButton');
        if (startButton) {
            startButton.addEventListener('click', function() {
                setTimeout(makeWhite, 100);
                setTimeout(makeWhite, 500);
                setTimeout(makeWhite, 1000);
            });
        }
    }, 500);
});
</script>
<!-- Outer white background wrapper -->
<div id="demo-white-wrapper" style="position: fixed; top: 0; left: 0; right: 0; bottom: 0; background-color: #FFFFFF; z-index: 0;"></div>

<!-- Main content with white background -->
<div class="container mx-auto py-10" style="background-color: #FFFFFF; position: relative; z-index: 1;">
    <h1 class="text-3xl font-bold mb-8 text-center" style="color: #FF0000;">PitchIQ Voice Demo</h1>
    
    <!-- Voice Agent Interface -->
    <div class="max-w-4xl mx-auto">
        <div class="bg-white rounded-lg shadow-lg p-6 mb-6" style="border-radius: 16px;">
            <!-- Status and Controls -->
            <div class="flex items-center justify-between mb-6">
                <div class="flex items-center space-x-4 w-full">
                    <span id="statusBadge" class="status-badge status-disconnected">Disconnected</span>
                    <div class="flex space-x-3 ml-auto">
                        <button id="startBtn" class="btn btn-primary" style="background-color: #FF0000; border-radius: 12px;">üé§ Start Conversation</button>
                        <button id="stopBtn" class="btn btn-secondary" disabled style="border-radius: 12px;">‚èπÔ∏è End Call</button>
                        <button id="muteBtn" class="btn btn-secondary" disabled style="border-radius: 12px;">üé§ Mute</button>
                    </div>
                </div>
            </div>
            
            <!-- Persona Generation React Component Mount Point -->
            <div id="persona-generation-container" class="mb-4" style="display: none;"></div>

            <!-- Persona Info Banner -->
            <div id="persona-info" class="mb-4 py-2 px-4 bg-gray-50 rounded-lg text-gray-800 font-medium" style="display: none; border-radius: 12px;">Speaking with: No persona yet</div>
            
            

            <!-- Hidden debug logs for development only -->
            <div class="hidden">
                <pre id="logs">Ready to start voice session...</pre>
            </div>
        </div>

        <!-- About Section -->
        <div class="bg-white rounded-lg shadow-sm p-6" style="border-radius: 16px;">
            <h2 class="text-xl font-bold mb-4" style="color: #FF0000;">About This Demo</h2>
            <p class="mb-4">PitchIQ's voice demo allows you to practice your sales skills with a dynamically generated persona. Simply start the conversation and begin speaking naturally.</p>
            
            <div class="grid md:grid-cols-2 gap-6">
                <div>
                    <h3 class="font-semibold mb-2" style="color: #333333;">How It Works</h3>
                    <ol class="list-decimal pl-5 space-y-1 text-sm">
                        <li>Click <strong>Start Conversation</strong> to begin</li>
                        <li>Describe your business when prompted</li>
                        <li>A custom persona will be generated for you</li>
                        <li>Practice your pitch with the AI prospect</li>
                        <li>Click <strong>End Call</strong> when finished</li>
                    </ol>
                </div>
                <div>
                    <h3 class="font-semibold mb-2" style="color: #333333;">Tips</h3>
                    <ul class="list-disc pl-5 space-y-1 text-sm">
                        <li>Speak clearly and at a natural pace</li>
                        <li>Allow brief pauses for the AI to respond</li>
                        <li>Use the <strong>Mute</strong> button if needed during the call</li>
                        <li>Try different approaches with the same persona</li>
                        <li>The conversation transcript is available during the call</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</div>
{% endblock %}

{% block footer_scripts %}
<style>
    .status-badge {
        padding: 4px 12px;
        border-radius: 20px;
        font-size: 12px;
        font-weight: bold;
        text-transform: uppercase;
    }
    
    .status-disconnected {
        background-color: #e5e7eb;
        color: #4b5563;
    }
    
    .status-connecting {
        background-color: #fef3c7;
        color: #d97706;
    }
    
    .status-active {
        background-color: #dcfce7;
        color: #16a34a;
    }
    
    .btn {
        padding: 8px 16px;
        border-radius: 6px;
        font-weight: 500;
        border: none;
        cursor: pointer;
        transition: all 0.2s;
    }
    
    .btn:disabled {
        opacity: 0.5;
        cursor: not-allowed;
    }
    
    .btn-primary {
        background-color: #FF0000;
        color: white;
    }
    
    .btn-primary:hover:not(:disabled) {
        background-color: #cc0000;
    }
    
    .btn-secondary {
        background-color: #6b7280;
        color: white;
    }
    
    .btn-secondary:hover:not(:disabled) {
        background-color: #4b5563;
    }
</style>

<script>
    // Voice Agent Implementation - Copied from working implementation
    let voiceAgentWS = null;
    let micStream = null;
    let micCtx = null;
    let micNode = null;
    let spkCtx = null;
    let playHead = 0;
    let connected = false;
    let connecting = false;
    let muted = false;
        let logs = [];

    // Persona generation variables
    let userProductInfo = {
        product: '',
        target_market: ''
    };
    let currentPersona = null;
    let isFirstInteraction = true;
    let personaGenerationStarted = false;

    // Smart logging variables
    let audioChunkCount = 0;
    let lastAudioLogTime = 0;
    let lastRMSValue = 0;
    let significantAudioEvents = 0;
    
    // TTS audio logging variables
    let ttsChunkCount = 0;
    let ttsTotalBytes = 0;
    let lastTTSLogTime = 0;
    let ttsStreamActive = false;
    
    // Constants
    const MIC_RATE = 48000;
    const TTS_RATE = 48000;
    const AUDIO_LOG_INTERVAL = 2000;
    const RMS_THRESHOLD = 100;
    const TTS_LOG_INTERVAL = 1000;

    // UI elements
    const statusBadge = document.getElementById('statusBadge');
    const startBtn = document.getElementById('startBtn');
    const stopBtn = document.getElementById('stopBtn');
    const muteBtn = document.getElementById('muteBtn');
    const logsArea = document.getElementById('logs');
    
    function log(message) {
        const timestamp = new Date().toLocaleTimeString();
        const logMessage = `${timestamp}  ${message}`;
        console.debug(logMessage);
        logs.push(logMessage);
        if (logs.length > 200) logs.shift();
        logsArea.textContent = logs.join('\n');
        logsArea.scrollTop = logsArea.scrollHeight;
    }
    
    function updateUI() {
        startBtn.disabled = connected || connecting;
        stopBtn.disabled = !connected;
        muteBtn.disabled = !connected;
        muteBtn.textContent = muted ? 'üîá Unmute' : 'üé§ Mute';
        
        if (connected) {
            statusBadge.className = 'status-badge status-active';
            statusBadge.textContent = 'Active Call';
        } else if (connecting) {
            statusBadge.className = 'status-badge status-connecting';
            statusBadge.textContent = 'Starting call...';
        } else {
            statusBadge.className = 'status-badge status-disconnected';
            statusBadge.textContent = 'Ready';
        }
    }
    
    // Audio worklet code (inline)
    const workletCode = `
        class DeepgramWorklet extends AudioWorkletProcessor {
            constructor() {
                super();
                this.sampleCount = 0;
            }

            process(inputs) {
                const input = inputs[0];
                if (!input || !input[0] || input[0].length === 0) {
                    return true;
                }

                const channelCount = input.length;
                const frameCount = input[0].length;
                
                const buffer = new Int16Array(frameCount);
                
                for (let frame = 0; frame < frameCount; frame++) {
                    let sample = 0;
                    
                    for (let channel = 0; channel < channelCount; channel++) {
                        sample += input[channel][frame];
                    }
                    
                    sample = sample / channelCount;
                    sample = sample * 1.4 * 32767;
                    buffer[frame] = Math.max(-32768, Math.min(32767, Math.round(sample)));
                }

                this.port.postMessage(buffer.buffer, [buffer.buffer]);
                this.sampleCount += frameCount;
                return true;
            }
        }
        registerProcessor('deepgram-worklet', DeepgramWorklet);
    `;
    
    async function startMicPump() {
        if (!micStream) return;

        micCtx = new AudioContext({ sampleRate: MIC_RATE });
        
        const blob = new Blob([workletCode], { type: 'application/javascript' });
        const workletUrl = URL.createObjectURL(blob);
        await micCtx.audioWorklet.addModule(workletUrl);
        
        if (!micCtx || micCtx.state === "closed") {
            log("‚ùå Audio context closed during worklet load - aborting mic setup");
            return;
        }
        
        micNode = new AudioWorkletNode(micCtx, "deepgram-worklet");

        let hold = new Int16Array(0);

        micNode.port.onmessage = (e) => {
            const data = e.data;
            if (muted || !voiceAgentWS || voiceAgentWS.readyState !== WebSocket.OPEN) return;

            const in16 = new Int16Array(data);
            let cat = new Int16Array(hold.length + in16.length);
            cat.set(hold);
            cat.set(in16, hold.length);

            const TARGET_SAMPLES = (MIC_RATE * 30) / 1000;
            while (cat.length >= TARGET_SAMPLES) {
                const chunk = cat.slice(0, TARGET_SAMPLES);
                
                const rms = Math.sqrt(chunk.reduce((sum, sample) => sum + sample * sample, 0) / chunk.length);
                const hasAudio = rms > RMS_THRESHOLD;
                
                audioChunkCount++;
                const now = Date.now();
                
                if (hasAudio) significantAudioEvents++;
                
                if (now - lastAudioLogTime >= AUDIO_LOG_INTERVAL) {
                    const avgRMS = Math.round(lastRMSValue);
                    const voiceActivity = significantAudioEvents > 0 ? 'üîä VOICE' : 'üîá quiet';
                    log(`üéôÔ∏è Audio: ${audioChunkCount} chunks (${(audioChunkCount * 30)}ms) | RMS: ${avgRMS} | ${voiceActivity} (${significantAudioEvents} active)`);
                    
                    audioChunkCount = 0;
                    significantAudioEvents = 0;
                    lastAudioLogTime = now;
                }
                
                lastRMSValue = rms;
                voiceAgentWS.send(chunk.buffer);
                cat = cat.slice(TARGET_SAMPLES);
            }
            hold = cat;
        };
        
        micCtx.createMediaStreamSource(micStream).connect(micNode);
        log(`üéôÔ∏è Mic ‚Üí DG Voice Agent @${MIC_RATE} Hz`);
        lastAudioLogTime = Date.now();
    }
    
    function initSpeaker() {
        spkCtx = new AudioContext({ 
            sampleRate: TTS_RATE,
            latencyHint: 'interactive'
        });
        playHead = spkCtx.currentTime + 0.1;
        spkCtx.resume().catch(() => {});
        log(`üîà Speaker ready @${TTS_RATE}Hz`);
    }
    
    async function playTTS(payload) {
        if (!spkCtx) {
            log("‚ùå No speaker context for TTS playback");
            return;
        }

        let pcmBuf = null;
        
        if (payload instanceof ArrayBuffer) {
            pcmBuf = payload;
        } else if (ArrayBuffer.isView(payload)) {
            pcmBuf = payload.buffer;
        } else if (payload instanceof Blob) {
            pcmBuf = await payload.arrayBuffer();
        } else {
            return;
        }

        if (!pcmBuf || pcmBuf.byteLength === 0) {
            return;
        }

        try {
            const i16 = new Int16Array(pcmBuf);
            const f32 = new Float32Array(i16.length);
            
            for (let i = 0; i < i16.length; i++) {
                f32[i] = (i16[i] / 32768) * 0.8;
            }
            
            const buf = spkCtx.createBuffer(1, f32.length, TTS_RATE);
            buf.copyToChannel(f32, 0);

            const src = spkCtx.createBufferSource();
            src.buffer = buf;
            
            const filter = spkCtx.createBiquadFilter();
            filter.type = 'lowpass';
            filter.frequency.value = 8000;
            filter.Q.value = 0.7;
            
            src.connect(filter);
            filter.connect(spkCtx.destination);

            const startAt = Math.max(playHead, spkCtx.currentTime + 0.05);
            src.start(startAt);
            playHead = startAt + buf.duration;
            
        
        log("üîß Fetching Deepgram API key...");
        const tokenRes = await fetch("/api/deepgram/token", { credentials: "include" });
        const tokenData = await tokenRes.json();
        
        if (!tokenData.success) {
            throw new Error(tokenData.error || 'Failed to get API key');
        }
        
        const wsUrl = `wss://agent.deepgram.com/v1/agent/converse`;
        log("üåê Connecting to Deepgram Voice Agent API...");
        
        voiceAgentWS = new WebSocket(wsUrl, ['token', tokenData.token]);
        
        voiceAgentWS.onopen = () => {
            log("‚úÖ WebSocket connected to Voice Agent");
            
            const config = {
                type: "Settings",
                audio: {
                    input: {
                        encoding: "linear16",
                        sample_rate: 48000
                    },
                    output: {
                        encoding: "linear16",
                        sample_rate: 48000,
                        container: "none"
                    }
                },
                agent: {
                    language: "en",
                    listen: {
                        provider: {
                            type: "deepgram",
                            model: "nova-2"
                        }
                    },
                    think: {
                        provider: {
                            type: "open_ai",
                            model: "gpt-4o-mini",
                            temperature: 0.8
                        },
                        prompt: "You are a helpful AI assistant. Keep responses natural, conversational, and brief. Speak as if you're having a friendly conversation."
                    },
                    speak: {
                        provider: {
                            type: "deepgram",
                            model: "aura-2-asteria-en"
                        }
                    },
                    greeting: "Hello! I'm your AI assistant. How can I help you today?"
                }
            };
            
            log("üì§ Sending configuration to Voice Agent");
            voiceAgentWS.send(JSON.stringify(config));
            
            initSpeaker();
            startMicPump();
            
            connected = true;
            connecting = false;
            updateUI();
        };
        
        voiceAgentWS.onmessage = (event) => {
            try {
                if (typeof event.data === 'string') {
                    const message = JSON.parse(event.data);
                    
                    if (message.type === 'ConversationText') {
                        const text = message.text || '';
                        const role = message.role || 'unknown';
                        
                        if (role === 'user') {
                            log(`üó£Ô∏è User: ${text}`);
                            processTranscriptMessage(`User: ${text}`);
                        } else if (role === 'assistant') {
                            log(`ü§ñ Agent: ${text}`);
                        }
                    } else if (message.type === 'SettingsApplied') {
                        log("‚öôÔ∏è Voice Agent configured successfully");
                    } else if (message.type === 'UserStartedSpeaking') {
                        log("üó£Ô∏è User speaking detected");
                    } else if (message.type === 'AgentThinking') {
                        log("ü§î Agent processing...");
                    } else if (message.type === 'Welcome') {
                        log("üëã Agent ready");
                    } else if (message.type === 'AgentAudioDone') {
                        log('üì° AgentAudioDone');
                        ttsStreamActive = false;
                        
                        if (ttsChunkCount > 0) {
                            const durationMs = (ttsTotalBytes / 2 / TTS_RATE * 1000).toFixed(0);
                            log(`üéµ TTS final: ${ttsChunkCount} chunks, ${(ttsTotalBytes / 1024).toFixed(1)}KB (${durationMs}ms audio)`);
                            ttsChunkCount = 0;
                            ttsTotalBytes = 0;
                        }
                    } else {
                        if (message.type === 'Warning' || message.type === 'Error') {
                            log(`‚ö†Ô∏è ${message.type}: ${JSON.stringify(message)}`);
                        } else {
                            log(`üì° ${message.type}`);
                        }
                    }
                } else {
                    const dataSize = event.data.byteLength || event.data.size || 0;
                    
                    if (dataSize > 0) {
                        ttsChunkCount++;
                        ttsTotalBytes += dataSize;
                        const now = Date.now();
                        
                        if (!ttsStreamActive) {
                            ttsStreamActive = true;
                            lastTTSLogTime = now;
                            log(`üîä TTS stream started`);
                        }
                        
                        if (now - lastTTSLogTime >= TTS_LOG_INTERVAL) {
                            const durationMs = (ttsTotalBytes / 2 / TTS_RATE * 1000).toFixed(0);
                            log(`üéµ TTS: ${ttsChunkCount} chunks, ${(ttsTotalBytes / 1024).toFixed(1)}KB (${durationMs}ms audio)`);
                            
                            ttsChunkCount = 0;
                            ttsTotalBytes = 0;
                            lastTTSLogTime = now;
                        }
                        
                        playTTS(event.data);
                    }
                }
            } catch (error) {
                log(`‚ùå Error processing message: ${error}`);
            }
        };
        
        voiceAgentWS.onerror = (error) => {
            log(`üö® WebSocket error: ${error}`);
        };
        
        voiceAgentWS.onclose = (event) => {
            log(`üåê WebSocket closed: ${event.code} ${event.reason}`);
            cleanup();
        };
        
    } catch (e) {
        log(`‚ùå Failed to start session: ${e}`);
        connecting = false;
        updateUI();
    }
}
    
function handlePersonaGenerated(persona) {
    currentPersona = persona;
    log(`‚úÖ Persona generated: ${persona.name} (${persona.role}) at ${persona.company}`);
    
    // Hide persona generation container
    document.getElementById('persona-generation-container').style.display = 'none';
    
    // Send auto-greeting in persona voice
    if (voiceAgentWS && voiceAgentWS.readyState === WebSocket.OPEN) {
        voiceAgentWS.send(JSON.stringify({ text: 'hello?' }));
    }

    // Show persona banner
    const banner = document.getElementById('persona-info');
    if (banner) {
        banner.textContent = `Speaking with: ${persona.name}, ${persona.role} at ${persona.company}`;
        banner.style.display = 'block';
    }
}
    
function handlePersonaError(error) {
    log(`‚ùå Persona generation error: ${error}`);
    alert(`Sorry, there was an error generating your prospect persona. Please try again.\n\nError: ${error}`);
    personaGenerationStarted = false;
    document.getElementById('persona-generation-container').style.display = 'none';
}

function startPersonaGeneration() {
    if (personaGenerationStarted || currentPersona) return;
    
    log("üßô Starting persona generation process...");
    personaGenerationStarted = true;
    
    // Show persona generation container
    const container = document.getElementById('persona-generation-container');
    if (container) {
        container.style.display = 'block';
    }
    
    // Render the PersonaGenerationCard React component
    renderPersonaGenerationComponent();
}

function cleanup() {
    try {
        if (micStream) {
            // Stop all tracks
            micStream.getTracks().forEach(track => {
                track.stop();
            });
            micStream = null;
        }
        if (micNode) {
            micNode.disconnect();
            micNode = null;
        }
        if (micCtx) {
            micCtx.close();
            micCtx = null;
        }
        if (spkCtx) {
            spkCtx.close();
            spkCtx = null;
        }
        if (voiceAgentWS) {
            voiceAgentWS.close();
            voiceAgentWS = null;
        }
    
        audioChunkCount = 0;
        lastAudioLogTime = 0;
        significantAudioEvents = 0;
        ttsChunkCount = 0;
        ttsTotalBytes = 0;
        lastTTSLogTime = 0;
        ttsStreamActive = false;
    
        connected = false;
        connecting = false;
        updateUI();
        
        log("‚úÖ Voice session ended");
    } catch (error) {
        console.error("Error during cleanup:", error);
        // Show user-friendly error
        alert("There was a problem ending the call. Please refresh the page to ensure all connections are closed properly.");
    }
}
    
    window.addEventListener('beforeunload', function() {
        if (connected) {
            stopSession();
        }
    });
    
    // Handle voice agent messages, including transcripts
    function processTranscriptMessage(text) {
        // Check for initial business information messages
        if (isFirstInteraction && text.includes("tell me about your") && text.includes("business")) {
            // This is likely the initial greeting asking about business
            isFirstInteraction = false;
        }
        
        // Look for business description in user's response
        if (!personaGenerationStarted && !currentPersona && text.includes("User:")) {
            const userMessage = text.split("User:").pop().trim();
            
            // If the user message is substantive (more than 20 characters), assume it's their business description
            if (userMessage && userMessage.length > 20) {
                // Extract product/service and target market from user's response
                // This is a simple heuristic - in production, you'd want more sophisticated parsing
                userProductInfo = {
                    product: userMessage.substring(0, 100),  // Use first part as product description
                    target_market: "businesses" // Default target market
                };
                
                // Start persona generation after a short delay
                setTimeout(() => {
                    startPersonaGeneration();
                }, 1000);
            }
        }
        
        
    }
    
    // Function to render the React component
    function renderPersonaGenerationComponent() {
        const container = document.getElementById('persona-generation-container');
        if (!container) return;
        
        const ProspectCallEventBus = {
            _handlers: {},
            _instance: null,
            getInstance() {
                if (!this._instance) {
                    this._instance = this;
                }
                return this._instance;
            },
            on(event, handler) {
                if (!this._handlers[event]) {
                    this._handlers[event] = [];
                }
                this._handlers[event].push(handler);
            },
            off(event, handler) {
                if (!this._handlers[event]) return;
                this._handlers[event] = this._handlers[event].filter(h => h !== handler);
            },
            emit(event, data) {
                if (!this._handlers[event]) return;
                this._handlers[event].forEach(handler => handler(data));
            }
        };
        
        // Create a root element
        const root = document.createElement('div');
        container.appendChild(root);
        
        // Render the component
        try {
            // Mock React.createElement and ReactDOM.render for the PersonaGenerationCard
            const personaGenElement = React.createElement('div', {className: 'persona-generation-card'},
                React.createElement('div', {className: 'relative h-16 w-16 flex items-center justify-center my-2 mx-auto'},
                    React.createElement('div', {className: 'absolute inset-0 flex items-center justify-center'},
                        React.createElement('div', {className: 'h-16 w-16 rounded-full border-2 border-t-red-500 animate-spin'})
                    ),
                    React.createElement('div', {className: 'absolute inset-0 flex items-center justify-center'},
                        React.createElement('span', {className: 'text-red-600'}, '‚ú®')
                    )
                ),
                React.createElement('div', {className: 'text-center w-full max-w-xs mx-auto'},
                    React.createElement('p', {className: 'text-base font-medium text-gray-800'}, 'Creating your perfect prospect...'),
                    React.createElement('div', {className: 'w-full bg-gray-100 rounded-full h-1 mt-2'},
                        React.createElement('div', {id: 'progress-bar', className: 'bg-red-500 h-1 rounded-full transition-all duration-300', style: {width: '0%'}})
                    )
                )
            );
            
            // Render to container
            ReactDOM.render(personaGenElement, root);
            
            // Simulate progress updates
            let progress = 0;
            const progressInterval = setInterval(() => {
                progress += 2;
                const progressBar = document.querySelector('#progress-bar');
                if (progressBar) progressBar.style.width = `${progress}%`;
                
                if (progress >= 100) {
                    clearInterval(progressInterval);
                    // Generate mock persona after full progress
                    setTimeout(() => generateMockPersona(), 500);
                }
            }, 100);
            
            // Mock persona generation
            function generateMockPersona() {
                // Create a realistic persona based on userProductInfo
                const industries = ['Technology', 'Healthcare', 'Finance', 'Manufacturing', 'Retail', 'Education'];
                const roles = ['Director of Operations', 'Chief Technology Officer', 'VP of Sales', 'Department Head', 'Senior Manager'];
                const names = ['Alex Johnson', 'Morgan Smith', 'Jordan Williams', 'Taylor Davis', 'Casey Brown'];
                const companies = ['Innovatech Solutions', 'Global Enterprises', 'Premier Services', 'NextGen Industries', 'Advance Systems'];
                
                const industry = industries[Math.floor(Math.random() * industries.length)];
                const role = roles[Math.floor(Math.random() * roles.length)];
                const name = names[Math.floor(Math.random() * names.length)];
                const company = companies[Math.floor(Math.random() * companies.length)];
                
                const persona = {
                    name,
                    role,
                    company,
                    industry,
                    primary_concern: 'Improving operational efficiency',
                    business_details: `${company} is a growing business in the ${industry} sector looking to optimize their processes.`,
                    about_person: `${name} is an experienced professional with over 10 years in the ${industry} industry.`,
                    pain_points: ['Inefficient operational processes', 'Increasing competition in the market', 'Rising costs and resource constraints'],
                    decision_factors: ['ROI', 'Implementation timeline', 'Integration with existing systems'],
                    communication_style: 'Straightforward and data-driven',
                    company_overview: `${company} specializes in providing solutions for the ${industry} industry with a focus on quality and innovation.`,
                    recent_milestones: 'Recently expanded operations to new markets',
                    strategic_priorities: 'Digital transformation and process optimization',
                    public_challenges: 'Adapting to rapid market changes',
                    surface_business_info: `A mid-sized company in the ${industry} sector with about 150 employees.`
                };
                
                // Pass the generated persona to the handler
                handlePersonaGenerated(persona);
            }
            
        } catch (error) {
            console.error('Error rendering PersonaGenerationCard:', error);
            handlePersonaError('Failed to render persona generation component');
        }
    }
</script>
{% endblock %} 