import React, { useEffect, useRef, useState, useCallback } from "react";
import { Button } from "@/components/ui/button";
import { useToast } from "@/hooks/use-toast";
import { createClient, AgentEvents } from "@deepgram/sdk";
import { Buffer } from "buffer";
import { useSimpleLog } from "@/hooks/useSimpleLog";
import { playHaptic } from "@/utils/haptics";
import { Mic, MicOff } from "lucide-react";

// Polyfill: DG browser SDK expects Node.Buffer in the global scope
if (typeof window !== "undefined" && !(window as any).Buffer) {
  (window as any).Buffer = Buffer;
}

const MIC_RATE = 48000; // Browser native rate
const TTS_RATE = 48000; // Keep aligned
const KEEPALIVE_MS = 8000; // ping to keep WS alive

// Global counter to track component instances
let componentInstanceCount = 0;
let globalSessionActive = false;

interface DeepgramAgent {
  configure: (settings: any) => void;
  start: () => void;
  send: (data: ArrayBuffer) => void;
  keepAlive: () => void;
  finish: () => void;
  close: () => void;
  on: (event: string, callback: (data?: any) => void) => void;
}

interface DeepgramClient {
  agent: () => DeepgramAgent;
}

const SamCoachAgent = () => {
  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ state */
  const [connected, setConnected] = useState(false);
  const [connecting, setConnecting] = useState(false);
  const [muted, setMuted] = useState(false);
  const [transcript, setTranscript] = useState("");
  const [sessionEnded, setSessionEnded] = useState(true); // Start as ended to show clean initial state
  const [inactivityWarning, setInactivityWarning] = useState(false);
  const [initializing, setInitializing] = useState(true);
  const [instanceId] = useState(() => {
    componentInstanceCount++;
    return componentInstanceCount;
  });
  
  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ refs */
  const dgRef = useRef<any>(null);
  const agentRef = useRef<any>(null);
  const micStream = useRef<MediaStream | null>(null);
  const micCtx = useRef<AudioContext | null>(null);
  const micNode = useRef<AudioWorkletNode | null>(null);
  const spkCtx = useRef<AudioContext | null>(null);
  const playHead = useRef<number>(0);
  const pingId = useRef<number | null>(null);
  const sdkInitialized = useRef<boolean>(false);
  const autoStarted = useRef<boolean>(false);
  const inactivityTimer = useRef<number | null>(null);
  const mutedRef = useRef<boolean>(false);

  const { toast } = useToast();
  const { messages, log, clearMessages } = useSimpleLog(`SamCoach#${instanceId}`);

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ token initialization */
  useEffect(() => {
    log(`ðŸ—ï¸ Component instance #${instanceId} mounted (total: ${componentInstanceCount})`);
    
    if (sdkInitialized.current) return;
    
    (async () => {
      try {
        log("ðŸ”§ Fetching DG token â€¦");
        const res = await fetch("/api/deepgram/token", { credentials: "include" });
        const { token } = await res.json();
        dgRef.current = createClient(token);
        sdkInitialized.current = true;
        log("âœ… SDK ready");
        setInitializing(false);
        
        // Auto-start session after SDK is ready
        if (!autoStarted.current && !connecting && !connected) {
          autoStarted.current = true;
          setTimeout(() => {
            if (!connecting && !connected) {
              startSession();
            }
          }, 500); // Small delay to ensure everything is ready
        }
      } catch (e) {
        log(`âŒ token â€“ ${e}`);
      }
    })();
  }, [log, instanceId]);

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ session management */
  const startSession = useCallback(async () => {
    log("â–¶ï¸ startSession invoked");
    if (connecting || connected || globalSessionActive) {
      log("âš ï¸ Session already active or connecting, skipping");
      return;
    }
    if (!dgRef.current) {
      log("ðŸš« Deepgram SDK not ready yet");
      return;
    }
    
    // Set global lock
    globalSessionActive = true;
    log(`ðŸ”’ Global session lock acquired by instance #${instanceId}`);
    
    // Force cleanup any existing agent
    if (agentRef.current) {
      log("âš ï¸ Agent already exists, cleaning up first");
      try {
        agentRef.current?.finish?.();
        agentRef.current?.close?.();
      } catch (e) {
        log(`âš ï¸ Error cleaning up existing agent: ${e}`);
      }
      agentRef.current = null;
    }
    
    // Reset states
    setConnected(false);
    setConnecting(true);
    setSessionEnded(false);

    try {
      
      // Request microphone
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 48000,
          channelCount: 1
        }
      });
      
      log("âœ… Mic granted");
      micStream.current = stream;
      agentRef.current = dgRef.current.agent();
      const a = agentRef.current;

      // Event handlers
      a.on("*", (evt: any) => {
        const eventType = evt?.type || "unknown";
        if (eventType !== "unknown") {
          log(`ðŸ“¡ DG â†’ ${eventType} ${eventType.includes('Audio') ? 'ðŸ”Š' : ''}`);
        }
      });

      a.on(AgentEvents.Open, () => {
        log("ðŸŒ WS open â†’ settings");
        log("ðŸ› ï¸ Settings payload â†’ " + JSON.stringify(buildSettings(), null, 2));
        a.configure(buildSettings());
        log("âœ… Settings sent - waiting for SettingsApplied...");
        
        initSpeaker();
        
        pingId.current = window.setInterval(() => {
          a.keepAlive();
        }, KEEPALIVE_MS);
        setConnected(true);
        setConnecting(false);
      });

      a.on(AgentEvents.SettingsApplied, () => {
        log("âœ… Settings ACK - starting mic");
        startMicPump();
        startInactivityTimer();
      });

      a.on(AgentEvents.UserStartedSpeaking, () => {
        log("ðŸ“¡ DG â†’ UserStartedSpeaking ðŸŽ¤");
        resetInactivityTimer();
      });

      a.on(AgentEvents.AgentAudioDone, () => {
        log("ðŸ“¡ DG â†’ AgentAudioDone ðŸ”‡");
      });

      a.on(AgentEvents.AgentThinking, () => {
        log("ðŸ“¡ DG â†’ AgentThinking ðŸ¤”");
      });

      a.on(AgentEvents.AgentStartedSpeaking, () => {
        log("ðŸ“¡ DG â†’ AgentStartedSpeaking ðŸ—£ï¸");
        resetInactivityTimer();
      });

      a.on(AgentEvents.ConversationText, (msg) => {
        log(`ðŸ“¡ DG â†’ ConversationText: "${msg.content}"`);
        setTranscript(msg.content);
        resetInactivityTimer();
      });
      
      a.on(AgentEvents.Audio, (payload) => {
        log(`ðŸ“¡ DG â†’ AgentAudio event received! ðŸ”Š`);
        // Only play audio if we have a valid audio context
        if (spkCtx.current && spkCtx.current.state !== "closed") {
          playTTS(payload);
        } else {
          log("âš ï¸ Ignoring audio - no valid audio context");
        }
      });

      a.on(AgentEvents.Error, (e) => {
        if (e.code === "CLIENT_MESSAGE_TIMEOUT") {
          log("â° Session timeout - no speech detected for a while");
        } else {
          log(`ðŸš¨ DG error ${JSON.stringify(e)}`);
        }
      });
      a.on(AgentEvents.Close, () => {
        log("ðŸŒ WS closed");
        cleanup();
      });
    } catch (e) {
      log(`âŒ startSession â€“ ${e}`);
      setConnecting(false);
    }
  }, [log, connecting, connected]);

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ settings */
  const buildSettings = () => ({
    audio: {
      input: {
        encoding: "linear16",
        sample_rate: 48000,
      },
      output: {
        encoding: "linear16",
        sample_rate: 48000,
      },
    },
    agent: {
      language: "en",
      listen: {
        provider: {
          type: "deepgram",
          model: "nova-2",
        },
      },
      think: {
        provider: {
          type: "open_ai",
          model: "gpt-4o-mini",
        },
      },
      speak: {
        provider: {
          type: "deepgram",
          model: "aura-2-asteria-en",
        },
      },
    },
    experimental: false,
  });

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ mic pump */
  const startMicPump = async () => {
    if (!micStream.current || !agentRef.current) {
      log("âŒ Missing mic stream or agent reference");
      return;
    }

    try {
      micCtx.current = new AudioContext({ sampleRate: MIC_RATE });
      await micCtx.current.audioWorklet.addModule("/static/deepgram-worklet.js");
      
      if (micCtx.current.state === "closed") {
        log("âŒ Audio context closed during worklet load - aborting mic setup");
        return;
      }
      
      micNode.current = new AudioWorkletNode(micCtx.current, "deepgram-worklet");
      log("âœ… Audio worklet node created successfully");
    } catch (error) {
      log(`âŒ Error setting up microphone: ${error}`);
      return;
    }

    let hold = new Int16Array(0);

    micNode.current.port.onmessage = (e) => {
      const data = e.data;
      
      // Check current mute state from ref (avoids stale closure)
      if (mutedRef.current) {
        // When muted, still process audio but don't send it
        return;
      }
      
      if (!agentRef.current) {
        log("âš ï¸ No agent reference, skipping audio data");
        return;
      }

      const in16 = new Int16Array(data);
      let cat = new Int16Array(hold.length + in16.length);
      cat.set(hold);
      cat.set(in16, hold.length);

      const TARGET_SAMPLES = (MIC_RATE * 30) / 1000; // 30ms chunks

      while (cat.length >= TARGET_SAMPLES) {
        const chunk = cat.slice(0, TARGET_SAMPLES);
        cat = cat.slice(TARGET_SAMPLES);

        try {
          agentRef.current.send(chunk.buffer);
        } catch (error) {
          log(`âŒ Error sending audio data: ${error}`);
          break;
        }
      }
      hold = cat;
    };

    const source = micCtx.current.createMediaStreamSource(micStream.current);
    source.connect(micNode.current);
    micNode.current.connect(micCtx.current.destination);
  };

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ speaker */
  const initSpeaker = () => {
    spkCtx.current = new AudioContext({ sampleRate: TTS_RATE });
    playHead.current = 0;
  };

  const playTTS = (payload: any) => {
    if (!spkCtx.current) {
      log("âŒ No speaker context for TTS playback");
      return;
    }

    // Check if audio context is closed - don't try to play audio
    if (spkCtx.current.state === "closed") {
      log("âš ï¸ Audio context is closed, skipping TTS playback");
      return;
    }

    // 1ï¸âƒ£ Normalize to ArrayBuffer - handle Uint8Array, Buffer, etc.
    const pcmBuf: ArrayBuffer | undefined =
      payload instanceof ArrayBuffer     ? payload :
      ArrayBuffer.isView(payload)        ? payload.buffer :
      ArrayBuffer.isView(payload?.audio) ? payload.audio.buffer :
      undefined;

    if (!pcmBuf?.byteLength) {
      log("âŒ Empty or invalid audio payload");
      return;
    }

    log(`ðŸ”‰ DG audio ${pcmBuf.byteLength / 2} samples (${pcmBuf.byteLength} bytes)`);
    
    // Check if audio context is suspended
    if (spkCtx.current.state === "suspended") {
      log("ðŸ”Š Resuming suspended audio context...");
      spkCtx.current.resume().then(() => {
        log("âœ… Audio context resumed");
      }).catch((err) => {
        log(`âŒ Failed to resume audio context: ${err}`);
      });
    }
    
    try {
      // 2ï¸âƒ£ Convert Int16 â†’ Float32 and schedule playback
      const i16 = new Int16Array(pcmBuf);
      const f32 = Float32Array.from(i16, (v) => v / 32768);
      const buf = spkCtx.current.createBuffer(1, f32.length, TTS_RATE);
      buf.copyToChannel(f32, 0);

      const src = spkCtx.current.createBufferSource();
      src.buffer = buf;
      src.connect(spkCtx.current.destination);

      const startAt = Math.max(playHead.current, spkCtx.current.currentTime + 0.02);
      src.start(startAt);
      playHead.current = startAt + buf.duration;
      
      log(`ðŸ”Š Scheduled TTS playback: ${buf.duration.toFixed(2)}s at ${startAt.toFixed(2)}s`);
      
      // Add event listeners to track playback
      src.onended = () => log("ðŸ”Š TTS playback completed");
      
    } catch (error) {
      log(`âŒ TTS playback failed: ${error}`);
    }
  };

  /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ cleanup */
  const cleanup = () => {
    micStream.current?.getTracks().forEach((t) => t.stop());
    micNode.current?.disconnect();
    
    // Only close audio contexts if they're not already closed
    if (micCtx.current && micCtx.current.state !== "closed") {
      micCtx.current.close().catch((e) => log(`âš ï¸ Error closing mic context: ${e}`));
    }
    if (spkCtx.current && spkCtx.current.state !== "closed") {
      spkCtx.current.close().catch((e) => log(`âš ï¸ Error closing speaker context: ${e}`));
    }
    
    if (pingId.current) clearInterval(pingId.current);
    
    // Clear inactivity timer
    if (inactivityTimer.current) {
      clearTimeout(inactivityTimer.current);
      inactivityTimer.current = null;
    }
    
    // Release global lock
    globalSessionActive = false;
    log(`ðŸ”“ Global session lock released by instance #${instanceId}`);
    
    setConnected(false);
    setConnecting(false);
  };

  const stopSession = useCallback(() => {
    log("ðŸ›‘ Stopping session...");
    
    // Set session ended immediately to stop processing new audio
    setSessionEnded(true);
    setConnected(false);
    setConnecting(false);
    setInactivityWarning(false);
    
    // Clear inactivity timer
    if (inactivityTimer.current) {
      clearTimeout(inactivityTimer.current);
      inactivityTimer.current = null;
    }
    
    // Clean up agent first
    if (agentRef.current) {
      try {
        agentRef.current?.finish?.();
        agentRef.current?.close?.();
      } catch (e) {
        log(`âš ï¸ Error stopping agent: ${e}`);
      }
      agentRef.current = null;
    }
    
    cleanup();
  }, [log]);

  const restartSession = useCallback(() => {
    if (connecting || connected) {
      log("âš ï¸ Session already active, cannot restart");
      return;
    }
    
    log("ðŸ”„ Restarting session...");
    setSessionEnded(false);
    setTranscript("");
    setMuted(false); // Reset mute state
    mutedRef.current = false; // Reset mute ref
    setInactivityWarning(false); // Reset warning state
    setInitializing(false); // Make sure we're not in initializing state
    autoStarted.current = false; // Reset auto-start flag
    
    // Small delay to ensure cleanup is complete
    setTimeout(() => {
      if (!connecting && !connected) {
        startSession();
      }
    }, 500);
  }, [log, startSession, connecting, connected]);

  const toggleMute = useCallback(() => {
    setMuted(prev => {
      const newMuted = !prev;
      mutedRef.current = newMuted; // Update ref immediately
      log(`ðŸŽ¤ ${newMuted ? 'Muted' : 'Unmuted'} microphone (instance #${instanceId})`);
      playHaptic("light");
      return newMuted;
    });
  }, [log, instanceId]);

  // Reset inactivity timer
  const resetInactivityTimer = useCallback(() => {
    if (inactivityTimer.current) {
      clearTimeout(inactivityTimer.current);
    }
    
    // Clear warning state
    setInactivityWarning(false);
    
    // Set warning at 45 seconds (15 seconds before timeout)
    setTimeout(() => {
      if (connected && !sessionEnded) {
        setInactivityWarning(true);
        log("âš ï¸ 15 seconds until session timeout due to inactivity");
      }
    }, 45000);
    
    inactivityTimer.current = window.setTimeout(() => {
      log("â° No speech activity for 1 minute - terminating session");
      stopSession();
    }, 60000); // 1 minute = 60,000ms
  }, [log, stopSession, connected, sessionEnded]);

  // Start inactivity timer when session becomes active
  const startInactivityTimer = useCallback(() => {
    log("â° Starting inactivity timer (1 minute)");
    resetInactivityTimer();
  }, [log, resetInactivityTimer]);

  const handleToggleSession = () => {
    playHaptic("medium");
    if (connected || connecting) {
      stopSession();
    } else {
      startSession();
    }
  };

  useEffect(() => {
    return () => {
      componentInstanceCount--;
      log(`ðŸ§¹ Component instance #${instanceId} unmounting (remaining: ${componentInstanceCount})`);
      stopSession();
    };
  }, [stopSession, log, instanceId]);

  const isSessionActive = connected || connecting;

  return (
    <div className="flex flex-col items-center justify-center h-full w-full p-4">
      {/* Clean UI - just the card with end call button */}
      <div className="w-full max-w-md p-6 bg-white border border-gray-900 rounded-xl shadow-lg hover:shadow-xl transition-all duration-200">
        {/* Connection Status Indicator */}
        <div className="flex items-center justify-center mb-6">
          <div className="flex items-center space-x-2">
            {initializing ? (
              <div className="w-12 h-12 border-4 border-red-500 border-t-transparent rounded-full animate-spin"></div>
            ) : (
              <>
                <div
                  className={`w-3 h-3 rounded-full ${
                    connected ? "bg-green-500" : connecting ? "bg-yellow-500 animate-pulse" : sessionEnded ? "bg-gray-400" : "bg-red-500"
                  }`}
                />
                <span className="text-sm font-medium text-gray-600">
                  {connected ? (muted ? "Connected (Muted)" : "Connected") : connecting ? "Connecting..." : sessionEnded ? "Session Ended" : "Disconnected"}
                </span>
              </>
            )}
          </div>
        </div>

        {/* Inactivity Warning */}
        {inactivityWarning && connected && (
          <div className="mb-4 p-3 bg-orange-50 border border-orange-200 rounded-lg">
            <div className="flex items-center space-x-2">
              <div className="w-2 h-2 bg-orange-500 rounded-full animate-pulse"></div>
              <span className="text-sm text-orange-700 font-medium">
                Session will end in 15 seconds due to inactivity
              </span>
            </div>
          </div>
        )}

        {/* Transcript Display */}
        {transcript && (
          <div className="mb-6 p-4 bg-gray-50 rounded-lg">
            <p className="text-sm text-gray-700">{transcript}</p>
          </div>
        )}

        {/* Action Buttons */}
        <div className="flex justify-center items-center space-x-3">
          {!initializing && !sessionEnded ? (
            <>
              {/* Mute Button */}
              <Button 
                onClick={toggleMute}
                size="sm"
                className={`w-10 h-10 p-0 rounded-xl border border-gray-900 font-medium transition-all duration-200 hover:scale-105 hover:shadow-lg ${
                  muted 
                    ? 'bg-red-100 hover:bg-red-200 text-red-700' 
                    : 'bg-white hover:bg-gray-100 text-black'
                }`}
                disabled={!isSessionActive}
                title={muted ? "Unmute" : "Mute"}
              >
                {muted ? <MicOff className="h-4 w-4" /> : <Mic className="h-4 w-4" />}
              </Button>
              
              {/* End Call Button */}
              <Button 
                onClick={() => {
                  playHaptic("medium");
                  stopSession();
                }}
                size="lg"
                className="px-8 py-3 bg-white hover:bg-red-100 text-black border border-gray-900 rounded-xl font-medium transition-all duration-200 hover:scale-105 hover:shadow-lg"
                disabled={!isSessionActive}
              >
                End Call
              </Button>
            </>
          ) : sessionEnded && !initializing ? (
            <Button 
              onClick={() => {
                playHaptic("medium");
                restartSession();
              }}
              size="lg"
              className="px-8 py-3 bg-white hover:bg-blue-100 text-black border border-gray-900 rounded-xl font-medium transition-all duration-200 hover:scale-105 hover:shadow-lg"
            >
              Start Again
            </Button>
          ) : null}
        </div>
      </div>
    </div>
  );
};

export default SamCoachAgent;